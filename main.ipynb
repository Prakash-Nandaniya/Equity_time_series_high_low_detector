{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbf02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/home/hp/Study/Code/AI/Detecting-reversal-point-in-equity-US/detecting-reversal-points-in-us-equities/competition_data/train.csv\"\n",
    "output_file = \"modified_train.csv\"\n",
    "\n",
    "# Function to process the CSV\n",
    "def process_csv(input_path, output_path):\n",
    "    with open(input_path, 'r', newline='') as infile, open(output_path, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Read the header row\n",
    "        header = next(reader)\n",
    "        \n",
    "        # Identify indices of columns with specific prefixes\n",
    "        happens_within_indices = [i for i, col in enumerate(header) if col.startswith('happens_within')]\n",
    "        occurs_within_indices = [i for i, col in enumerate(header) if col.startswith('occurs_within')]\n",
    "        crossing_above_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_above')]\n",
    "        crossing_below_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_below')]\n",
    "        trending_down_indices = [i for i, col in enumerate(header) if col.startswith('trending_down_and_above')]\n",
    "        trending_up_indices = [i for i, col in enumerate(header) if col.startswith('trending_up_and_below')]\n",
    "        troughs_indices = [i for i, col in enumerate(header) if col.startswith('troughs')]\n",
    "        zone_indices = [i for i, col in enumerate(header) if col.startswith('zone')]\n",
    "        peaks_indices = [i for i, col in enumerate(header) if col.startswith('peaks')]\n",
    "        \n",
    "        # Keep columns that are not any of the specified prefixes\n",
    "        other_indices = [i for i, col in enumerate(header) if not (\n",
    "            col.startswith('happens_within') or \n",
    "            col.startswith('occurs_within') or \n",
    "            col.startswith('cross_threshold_from_above') or \n",
    "            col.startswith('cross_threshold_from_below') or\n",
    "            col.startswith('trending_down_and_above') or\n",
    "            col.startswith('trending_up_and_below') or\n",
    "            col.startswith('troughs') or\n",
    "            col.startswith('zone') or\n",
    "            col.startswith('peaks')\n",
    "        )]\n",
    "        \n",
    "        # Create new header: keep other columns, add new columns for counts\n",
    "        new_header = [header[i] for i in other_indices] + [\n",
    "            'happens_within', \n",
    "            'occurs_within', \n",
    "            'cross_threshold_from_above', \n",
    "            'cross_threshold_from_below',\n",
    "            'trending_down_and_above',\n",
    "            'trending_up_and_below',\n",
    "            'troughs',\n",
    "            'zone',\n",
    "            'peaks'\n",
    "        ]\n",
    "        writer.writerow(new_header)\n",
    "        \n",
    "        # Process each data row\n",
    "        for row in reader:\n",
    "            # Count \"True\" values in each set of columns\n",
    "            happens_within_count = sum(1 for i in happens_within_indices if row[i] == 'True')\n",
    "            occurs_within_count = sum(1 for i in occurs_within_indices if row[i] == 'True')\n",
    "            crossing_above_count = sum(1 for i in crossing_above_indices if row[i] == 'True')\n",
    "            crossing_below_count = sum(1 for i in crossing_below_indices if row[i] == 'True')\n",
    "            trending_down_count = sum(1 for i in trending_down_indices if row[i] == 'True')\n",
    "            trending_up_count = sum(1 for i in trending_up_indices if row[i] == 'True')\n",
    "            troughs_count = sum(1 for i in troughs_indices if row[i] == 'True')\n",
    "            zone_count = sum(1 for i in zone_indices if row[i] == 'True')\n",
    "            peaks_count = sum(1 for i in peaks_indices if row[i] == 'True')\n",
    "            \n",
    "            # Create new row: keep other columns, add counted columns\n",
    "            new_row = [row[i] for i in other_indices] + [\n",
    "                str(happens_within_count), \n",
    "                str(occurs_within_count), \n",
    "                str(crossing_above_count), \n",
    "                str(crossing_below_count),\n",
    "                str(trending_down_count),\n",
    "                str(trending_up_count),\n",
    "                str(troughs_count),\n",
    "                str(zone_count),\n",
    "                str(peaks_count)\n",
    "            ]\n",
    "            writer.writerow(new_row)\n",
    "\n",
    "# Execute the processing\n",
    "process_csv(input_file, output_file)\n",
    "print(f\"Processed CSV saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe0f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"/home/hp/Study/Code/AI/Detecting-reversal-point-in-equity-US/detecting-reversal-points-in-us-equities/competition_data/test.csv\"\n",
    "output_file = \"modified_test.csv\"\n",
    "\n",
    "# Function to process the CSV\n",
    "def process_csv(input_path, output_path):\n",
    "    with open(input_path, 'r', newline='') as infile, open(output_path, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Read the header row\n",
    "        header = next(reader)\n",
    "        \n",
    "        # Identify indices of columns with specific prefixes\n",
    "        happens_within_indices = [i for i, col in enumerate(header) if col.startswith('happens_within')]\n",
    "        occurs_within_indices = [i for i, col in enumerate(header) if col.startswith('occurs_within')]\n",
    "        crossing_above_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_above')]\n",
    "        crossing_below_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_below')]\n",
    "        trending_down_indices = [i for i, col in enumerate(header) if col.startswith('trending_down_and_above')]\n",
    "        trending_up_indices = [i for i, col in enumerate(header) if col.startswith('trending_up_and_below')]\n",
    "        troughs_indices = [i for i, col in enumerate(header) if col.startswith('troughs')]\n",
    "        zone_indices = [i for i, col in enumerate(header) if col.startswith('zone')]\n",
    "        peaks_indices = [i for i, col in enumerate(header) if col.startswith('peaks')]\n",
    "        \n",
    "        # Keep columns that are not any of the specified prefixes\n",
    "        other_indices = [i for i, col in enumerate(header) if not (\n",
    "            col.startswith('happens_within') or \n",
    "            col.startswith('occurs_within') or \n",
    "            col.startswith('cross_threshold_from_above') or \n",
    "            col.startswith('cross_threshold_from_below') or\n",
    "            col.startswith('trending_down_and_above') or\n",
    "            col.startswith('trending_up_and_below') or\n",
    "            col.startswith('troughs') or\n",
    "            col.startswith('zone') or\n",
    "            col.startswith('peaks')\n",
    "        )]\n",
    "        \n",
    "        # Create new header: keep other columns, add new columns for counts\n",
    "        new_header = [header[i] for i in other_indices] + [\n",
    "            'happens_within', \n",
    "            'occurs_within', \n",
    "            'cross_threshold_from_above', \n",
    "            'cross_threshold_from_below',\n",
    "            'trending_down_and_above',\n",
    "            'trending_up_and_below',\n",
    "            'troughs',\n",
    "            'zone',\n",
    "            'peaks'\n",
    "        ]\n",
    "        writer.writerow(new_header)\n",
    "        \n",
    "        # Process each data row\n",
    "        for row in reader:\n",
    "            # Count \"True\" values in each set of columns\n",
    "            happens_within_count = sum(1 for i in happens_within_indices if row[i] == 'True')\n",
    "            occurs_within_count = sum(1 for i in occurs_within_indices if row[i] == 'True')\n",
    "            crossing_above_count = sum(1 for i in crossing_above_indices if row[i] == 'True')\n",
    "            crossing_below_count = sum(1 for i in crossing_below_indices if row[i] == 'True')\n",
    "            trending_down_count = sum(1 for i in trending_down_indices if row[i] == 'True')\n",
    "            trending_up_count = sum(1 for i in trending_up_indices if row[i] == 'True')\n",
    "            troughs_count = sum(1 for i in troughs_indices if row[i] == 'True')\n",
    "            zone_count = sum(1 for i in zone_indices if row[i] == 'True')\n",
    "            peaks_count = sum(1 for i in peaks_indices if row[i] == 'True')\n",
    "            \n",
    "            # Create new row: keep other columns, add counted columns\n",
    "            new_row = [row[i] for i in other_indices] + [\n",
    "                str(happens_within_count), \n",
    "                str(occurs_within_count), \n",
    "                str(crossing_above_count), \n",
    "                str(crossing_below_count),\n",
    "                str(trending_down_count),\n",
    "                str(trending_up_count),\n",
    "                str(troughs_count),\n",
    "                str(zone_count),\n",
    "                str(peaks_count)\n",
    "            ]\n",
    "            writer.writerow(new_row)\n",
    "\n",
    "# Execute the processing\n",
    "process_csv(input_file, output_file)\n",
    "print(f\"Processed CSV saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a2ef0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'modified_train.csv' has been sorted by column 't' and overwritten.\n",
      "File 'modified_test.csv' has been sorted by column 't' and overwritten.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sort_csv_by_column(file_path, column_name):\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Sort by column 't'\n",
    "    df_sorted = df.sort_values(by=column_name)\n",
    "\n",
    "    # Write back to same file (overwrite)\n",
    "    df_sorted.to_csv(file_path, index=False)\n",
    "\n",
    "    print(f\"File '{file_path}' has been sorted by column '{column_name}' and overwritten.\")\n",
    "\n",
    "sort_csv_by_column('modified_train.csv', 't')\n",
    "sort_csv_by_column('modified_test.csv', 't')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a34a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train_id  ticker_id           t class_label   troughs     peaks  \\\n",
      "1        410          1  2023-04-03           N -1.142867 -1.146422   \n",
      "10      1688          1  2023-04-05           N -1.142867 -1.146422   \n",
      "13      1293          1  2023-04-06           N -1.142867 -1.146422   \n",
      "20      1174          1  2023-04-11           N -1.142867 -1.146422   \n",
      "24      1450          1  2023-04-12           N -1.142867 -1.146422   \n",
      "\n",
      "    momentum_ratio      within  trending  trending_momentum_boosted  \\\n",
      "1        -2.867840 -226.413043 -1.721986                  -8.802801   \n",
      "10       -4.893375 -258.413043 -1.721986                  -8.802801   \n",
      "13       -3.740780 -258.413043 -1.721986                  -8.802801   \n",
      "20       -2.038810 -250.413043  1.578574                   6.171519   \n",
      "24       -1.459645   21.586957 -1.721986                  -8.802801   \n",
      "\n",
      "    within_momentum_boosted  trailing_momentum_ratio  \n",
      "1             -90609.037187                -2.850005  \n",
      "10           -103386.441275                -3.722823  \n",
      "13           -103384.292571                -3.720169  \n",
      "20           -100183.425227                -2.745459  \n",
      "24              8577.221877                -1.995614  \n",
      "Training model for ticker_id 1\n",
      "df shape: (322, 12)\n",
      "X shape: (322, 8), y shape: (322,)\n",
      "Confusion Matrix:\n",
      "    H  L    N\n",
      "H  11  0    0\n",
      "L   0  7    0\n",
      "N   0  0  304\n",
      "\n",
      "Evaluation Metrics:\n",
      "Micro F1 Score: 1.0000\n",
      "Macro F1 Score: 1.0000\n",
      "Macro Balanced Accuracy: 1.0000\n",
      "Matthews Correlation Coefficient: 1.0000\n",
      "Inference Runtime (seconds): 0.0210\n",
      "Training model for ticker_id 2\n",
      "df shape: (322, 12)\n",
      "X shape: (322, 8), y shape: (322,)\n",
      "Confusion Matrix:\n",
      "   H  L    N\n",
      "H  8  0    0\n",
      "L  0  6    0\n",
      "N  0  1  307\n",
      "\n",
      "Evaluation Metrics:\n",
      "Micro F1 Score: 0.9969\n",
      "Macro F1 Score: 0.9738\n",
      "Macro Balanced Accuracy: 0.9989\n",
      "Matthews Correlation Coefficient: 0.9650\n",
      "Inference Runtime (seconds): 0.0157\n",
      "Training model for ticker_id 3\n",
      "df shape: (322, 12)\n",
      "X shape: (322, 8), y shape: (322,)\n",
      "Confusion Matrix:\n",
      "    H   L    N\n",
      "H  10   0    0\n",
      "L   0  13    0\n",
      "N   0   0  299\n",
      "\n",
      "Evaluation Metrics:\n",
      "Micro F1 Score: 1.0000\n",
      "Macro F1 Score: 1.0000\n",
      "Macro Balanced Accuracy: 1.0000\n",
      "Matthews Correlation Coefficient: 1.0000\n",
      "Inference Runtime (seconds): 0.0184\n",
      "Training model for ticker_id 4\n",
      "df shape: (322, 12)\n",
      "X shape: (322, 8), y shape: (322,)\n",
      "Confusion Matrix:\n",
      "   H  L    N\n",
      "H  9  0    0\n",
      "L  0  6    0\n",
      "N  0  0  307\n",
      "\n",
      "Evaluation Metrics:\n",
      "Micro F1 Score: 1.0000\n",
      "Macro F1 Score: 1.0000\n",
      "Macro Balanced Accuracy: 1.0000\n",
      "Matthews Correlation Coefficient: 1.0000\n",
      "Inference Runtime (seconds): 0.0145\n",
      "Training model for ticker_id 5\n",
      "df shape: (322, 12)\n",
      "X shape: (322, 8), y shape: (322,)\n",
      "Confusion Matrix:\n",
      "    H   L    N\n",
      "H  11   0    0\n",
      "L   0  10    0\n",
      "N   0   0  301\n",
      "\n",
      "Evaluation Metrics:\n",
      "Micro F1 Score: 1.0000\n",
      "Macro F1 Score: 1.0000\n",
      "Macro Balanced Accuracy: 1.0000\n",
      "Matthews Correlation Coefficient: 1.0000\n",
      "Inference Runtime (seconds): 0.0161\n",
      "Training model for ticker_id 6\n",
      "df shape: (322, 12)\n",
      "X shape: (322, 8), y shape: (322,)\n",
      "Confusion Matrix:\n",
      "   H   L    N\n",
      "H  9   0    0\n",
      "L  0  12    0\n",
      "N  0   0  301\n",
      "\n",
      "Evaluation Metrics:\n",
      "Micro F1 Score: 1.0000\n",
      "Macro F1 Score: 1.0000\n",
      "Macro Balanced Accuracy: 1.0000\n",
      "Matthews Correlation Coefficient: 1.0000\n",
      "Inference Runtime (seconds): 0.0219\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score, matthews_corrcoef\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import xgboost as xgb\n",
    "import time\n",
    "\n",
    "# File path for train\n",
    "input_file = \"modified_train.csv\"\n",
    "\n",
    "# Load the CSV into a Pandas DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Simplify class labels\n",
    "df['class_label'] = df['class_label'].fillna('N')\n",
    "df[\"class_label\"] = df[\"class_label\"].replace({\"HH\": \"H\", \"LH\": \"H\", \"LL\": \"L\", \"HL\": \"L\", '': 'N'})\n",
    "\n",
    "# Split into separate DataFrames based on ticker_id\n",
    "df_001 = df[df['ticker_id'] == 1].copy()\n",
    "df_002 = df[df['ticker_id'] == 2].copy()\n",
    "df_003 = df[df['ticker_id'] == 3].copy() \n",
    "df_004 = df[df['ticker_id'] == 4].copy()\n",
    "df_005 = df[df['ticker_id'] == 5].copy()\n",
    "df_006 = df[df['ticker_id'] == 6].copy()\n",
    "\n",
    "\n",
    "# Columns to average (specific to each DataFrame)\n",
    "average_cols = ['troughs', 'peaks','momentum_ratio', 'within', 'trending', 'trending_momentum_boosted','within_momentum_boosted', 'trailing_momentum_ratio']\n",
    "\n",
    "#drop columns\n",
    "drop_cols = ['occurs_within', 'happens_within','cross_threshold_from_above', 'cross_threshold_from_below','trending_down_and_above', 'trending_up_and_below','zone', 'momentum', 'ratio', 'sm_momentum', 'sm_ratio']\n",
    "\n",
    "# Columns for exponential transformation\n",
    "exp_cols = ['momentum_ratio', 'trailing_momentum_ratio', 'peaks', 'troughs', 'trending', 'trending_momentum_boosted']\n",
    "\n",
    "# Function to process each train DataFrame and store means and exp_cols\n",
    "def process_dataframe(df):\n",
    "\n",
    "    # Feature engineering\n",
    "    df['momentum_ratio'] = df['momentum'] + df['ratio'] + df['sm_momentum'] + df['sm_ratio']\n",
    "    df['within'] = df['occurs_within'] + df['happens_within']\n",
    "    df['trending'] = df['trending_down_and_above'] + df['trending_up_and_below']\n",
    "    df[\"trending_momentum_boosted\"] = df[\"trending\"] * df[\"momentum_ratio\"] *0.01\n",
    "    df[\"within_momentum_boosted\"] = df[\"within\"] * abs(df[\"momentum_ratio\"])\n",
    "    df['trailing_momentum_ratio'] = df['momentum_ratio'].ewm(alpha=0.5, adjust=False).mean()\n",
    "    \n",
    "    # Drop unused columns\n",
    "    df.drop(columns=drop_cols, inplace=True, errors='ignore')\n",
    "    \n",
    "    # Compute and store means\n",
    "    means = df[average_cols].mean().to_dict()\n",
    "\n",
    "    # Centralize columns by subtracting mean (per DataFrame)\n",
    "    df[average_cols] = df[average_cols] - df[average_cols].mean()\n",
    "\n",
    "    # Apply exponential transformation\n",
    "    for col in exp_cols:\n",
    "        df[col] = np.sign(df[col]) * np.exp(np.abs(df[col]))\n",
    "\n",
    "    # Sort by time column 't'\n",
    "    df.sort_values(by=['t'], inplace=True)\n",
    "\n",
    "    return df, means\n",
    "\n",
    "# Apply processing to each DataFrame and store means and exp_cols\n",
    "df_001, means_001 = process_dataframe(df_001)\n",
    "df_002, means_002 = process_dataframe(df_002)\n",
    "df_003, means_003 = process_dataframe(df_003)\n",
    "df_004, means_004 = process_dataframe(df_004)\n",
    "df_005, means_005 = process_dataframe(df_005)\n",
    "df_006, means_006 = process_dataframe(df_006)\n",
    "\n",
    "print(df_001.head())\n",
    "\n",
    "# Function to train XGBoost model\n",
    "def train_model(df, ticker_id):\n",
    "    features = ['troughs', 'peaks', 'momentum_ratio', 'trailing_momentum_ratio', 'within', 'trending','trending_momentum_boosted', 'within_momentum_boosted']\n",
    "    # Ensure features exist\n",
    "    features = [f for f in features if f in df.columns]\n",
    "    \n",
    "    print(f\"Training model for ticker_id {ticker_id}\")\n",
    "    print(f\"df shape: {df.shape}\")\n",
    "    \n",
    "    X = df[features].values.astype(np.float32)\n",
    "    y = df['class_label']\n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y_enc = le.fit_transform(y)  # H=0, L=1, N=2\n",
    "\n",
    "    # Verify feature count\n",
    "    num_features = len(features)\n",
    "    if X.shape[1] != num_features:\n",
    "        print(f\"Error: X has {X.shape[1]} columns, expected {num_features}\")\n",
    "\n",
    "    # Compute class weights based on full data\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_enc), y=y_enc)\n",
    "    weight_dict = {cls: weight for cls, weight in zip(np.unique(y_enc), class_weights)}\n",
    "    sw = np.array([weight_dict[label] for label in y_enc])\n",
    "\n",
    "    # Convert data to DMatrix for XGBoost train API\n",
    "    dtrain = xgb.DMatrix(X, label=y_enc, weight=sw)\n",
    "\n",
    "    # Define parameters\n",
    "    params = {\n",
    "        'objective': 'multi:softprob',\n",
    "        'num_class': 3,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 3,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # Train model on full dataset\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=7000,\n",
    "        verbose_eval=True\n",
    "    )\n",
    "\n",
    "    # Evaluate on full dataset (for reference)\n",
    "    dtest = xgb.DMatrix(X)\n",
    "\n",
    "    # Measure inference runtime\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(dtest)\n",
    "    y_pred = np.argmax(y_pred, axis=1)  # Convert probabilities to class labels\n",
    "    inference_time = time.time() - start_time\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_enc, y_pred)\n",
    "    cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm_df)\n",
    "\n",
    "    # Compute evaluation metrics\n",
    "    micro_f1 = f1_score(y_enc, y_pred, average='micro')\n",
    "    macro_f1 = f1_score(y_enc, y_pred, average='macro')\n",
    "    balanced_acc = balanced_accuracy_score(y_enc, y_pred)\n",
    "    mcc = matthews_corrcoef(y_enc, y_pred)\n",
    "\n",
    "    # Print evaluation metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    print(f\"Micro F1 Score: {micro_f1:.4f}\")\n",
    "    print(f\"Macro F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"Macro Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
    "    print(f\"Inference Runtime (seconds): {inference_time:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train models for each ticker\n",
    "model_001 = train_model(df_001, 1)\n",
    "model_002 = train_model(df_002, 2)\n",
    "model_003 = train_model(df_003, 3)\n",
    "model_004 = train_model(df_004, 4)\n",
    "model_005 = train_model(df_005, 5)\n",
    "model_006 = train_model(df_006, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461118c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission_file.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load test data\n",
    "test_file = \"modified_test.csv\"\n",
    "\n",
    "# Create feature-to-index mapping by reading the first row of the CSV\n",
    "def create_feature_mapping(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        first_row = f.readline().strip()\n",
    "    feature_names = first_row.split(',')\n",
    "    feature_to_index = {name: idx for idx, name in enumerate(feature_names)}\n",
    "    return feature_to_index\n",
    "\n",
    "feature_to_index = create_feature_mapping(test_file)\n",
    "\n",
    "# Initialize EWMA state for each ticker\n",
    "ewma_state = {1: None, 2: None, 3: None, 4: None, 5: None, 6: None}\n",
    "alpha = 0.5\n",
    "\n",
    "# Collect predictions\n",
    "submission_data = []\n",
    "\n",
    "# Process test data row by row\n",
    "with open(test_file, 'r') as f:\n",
    "    # Skip the header row\n",
    "    next(f)\n",
    "    \n",
    "    for line in f:\n",
    "        # Split the row into values\n",
    "        values = line.strip().split(',')\n",
    "        \n",
    "        # Create dictionary mapping feature names to values\n",
    "        row_dict = {}\n",
    "        for feature, idx in feature_to_index.items():\n",
    "            value = values[idx]\n",
    "            if feature == 't':\n",
    "                continue\n",
    "            else:\n",
    "                row_dict[feature] = float(value) if value.strip() else 0.0\n",
    "        \n",
    "        # Get ticker_id and id\n",
    "        ticker_id = int(row_dict['ticker_id'])\n",
    "        id_val = row_dict['id']\n",
    "        \n",
    "        # Select model, label encoder, and means based on ticker_id\n",
    "        if ticker_id == 1:\n",
    "            means = means_001\n",
    "            model = model_001\n",
    "        elif ticker_id == 2:\n",
    "            means = means_002\n",
    "            model = model_002\n",
    "        elif ticker_id == 3:\n",
    "            means = means_003\n",
    "            model = model_003\n",
    "        elif ticker_id == 4:\n",
    "            means = means_004\n",
    "            model = model_004\n",
    "        elif ticker_id == 5:\n",
    "            means = means_005\n",
    "            model = model_005\n",
    "        elif ticker_id == 6:\n",
    "            means = means_006\n",
    "            model = model_006\n",
    "        else:\n",
    "            submission_data.append({'id': id_val, 'class_label': 'N'})\n",
    "            continue\n",
    "        \n",
    "        # Feature engineering\n",
    "        row_dict['momentum_ratio'] = (row_dict['momentum'] + row_dict['ratio'] + row_dict['sm_momentum'] + row_dict['sm_ratio'])\n",
    "        row_dict['within'] = (row_dict.get('occurs_within', 0.0) + row_dict.get('happens_within', 0.0))\n",
    "        row_dict['trending'] = (row_dict.get('trending_down_and_above', 0.0) + row_dict.get('trending_up_and_below', 0.0))\n",
    "        row_dict['trending_momentum_boosted'] = row_dict['trending'] * row_dict['momentum_ratio']\n",
    "        row_dict['within_momentum_boosted'] = row_dict['within'] * abs(row_dict['momentum_ratio'])\n",
    "        # Calculate trailing_momentum_ratio using EWMA\n",
    "        if ewma_state[ticker_id] is None:\n",
    "            row_dict['trailing_momentum_ratio'] = row_dict['momentum_ratio']\n",
    "        else:\n",
    "            row_dict['trailing_momentum_ratio'] = (alpha * row_dict['momentum_ratio'] + (1 - alpha) * ewma_state[ticker_id])\n",
    "        ewma_state[ticker_id] = row_dict['trailing_momentum_ratio']\n",
    "        \n",
    "        # Centralize features using training means\n",
    "        for col in average_cols:\n",
    "            row_dict[col] -= means[col]\n",
    "        \n",
    "        # Apply exponential transformation with clipping to avoid inf\n",
    "        for col in exp_cols:\n",
    "            val = row_dict[col]\n",
    "            row_dict[col] = np.sign(val) * np.exp(np.clip(np.abs(val), 0, 20))\n",
    "        \n",
    "        # Drop unused columns\n",
    "        for col in drop_cols:\n",
    "            row_dict.pop(col, None)\n",
    "        \n",
    "        # Select features for prediction\n",
    "        features = ['troughs', 'peaks', 'momentum_ratio', 'trailing_momentum_ratio', 'within', 'trending', 'trending_momentum_boosted', 'within_momentum_boosted']\n",
    "        \n",
    "        # Prepare X for this row\n",
    "        X_row = np.array([[row_dict[f] for f in features]], dtype=np.float32)\n",
    "        \n",
    "        dtest = xgb.DMatrix(X_row)\n",
    "        \n",
    "        # Get predictions\n",
    "        y_pred = model.predict(dtest)\n",
    "        #print(y_pred)  # Uncomment for debugging\n",
    "        \n",
    "        # Apply custom prediction logic\n",
    "        prob = y_pred[0]  # Single row\n",
    "        h_prob = prob[0]  # Probability for H\n",
    "        l_prob = prob[1]  # Probability for L\n",
    "        n_prob = prob[2]  # Probability for N\n",
    "        # if h_prob >= 0.93:\n",
    "        #     label = 'H'\n",
    "        # elif l_prob >= 0.93:\n",
    "        #     label = 'L'\n",
    "        # else:\n",
    "        #     label = 'N'\n",
    "\n",
    "        if h_prob > l_prob and h_prob > n_prob:\n",
    "            label = 'H'\n",
    "        elif l_prob > h_prob and l_prob > n_prob:\n",
    "            label = 'L'\n",
    "        else:\n",
    "            label = 'N'\n",
    "\n",
    "        # Add to submission\n",
    "        submission_data.append({'id': id_val, 'class_label': label})\n",
    "\n",
    "# Create submission DataFrame and save as CSV\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "submission_df.to_csv('predictions.csv', index=False, sep=',')\n",
    "print(\"Submission file created: predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cabdcf",
   "metadata": {},
   "source": [
    "### tasks:\n",
    "(1) best num_boost_riund - hiher less H and L, \n",
    "(2) best probability threshold, \n",
    "(3) whihc is better probbaility threshold or just return based on max\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
