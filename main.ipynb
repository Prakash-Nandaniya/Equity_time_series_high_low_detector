{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "0d027728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # webscraping_yahoo finance data\n",
    "\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# import time\n",
    "\n",
    "# chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# driver = webdriver.Chrome(options=chrome_options)\n",
    "# url = \"https://finance.yahoo.com/quote/AAPL/history/?period1=345479400&period2=1761924776\"\n",
    "\n",
    "# driver.get(url)\n",
    "# time.sleep(5)  # Wait for the dynamic table to load\n",
    "\n",
    "# soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "# # Correct table class selector based on your screenshot\n",
    "# table = soup.find('table', {'class': 'table yf-1jecxey noDl hideOnPrint'})\n",
    "# if table:\n",
    "#     # Parse headers\n",
    "#     headers = []\n",
    "#     thead = table.find('thead')\n",
    "#     if thead:\n",
    "#         for th in thead.find_all('th'):\n",
    "#             headers.append(th.text.strip())\n",
    "#     # If headers list is empty, try checking the first tr's td\n",
    "#     if not headers and thead:\n",
    "#         first_tr = thead.find('tr')\n",
    "#         if first_tr:\n",
    "#             for td in first_tr.find_all('td'):\n",
    "#                 headers.append(td.text.strip())\n",
    "\n",
    "#     # Parse rows\n",
    "#     rows = []\n",
    "#     tbody = table.find('tbody')\n",
    "#     for tr in tbody.find_all('tr'):\n",
    "#         row = []\n",
    "#         for td in tr.find_all('td'):\n",
    "#             row.append(td.text.strip())\n",
    "#         if row:\n",
    "#             rows.append(row)\n",
    "\n",
    "#     # Save to CSV\n",
    "#     df = pd.DataFrame(rows, columns=headers if headers else None)\n",
    "#     df.to_csv('yahoo_finance_AAPL_historical.csv', index=False)\n",
    "#     print(\"CSV file saved as yahoo_finance_AAPL_historical.csv.\")\n",
    "# else:\n",
    "#     print(\"Table not found.\")\n",
    "\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "0b9b8a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cleaning file\n",
    "\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# # Read the CSV file\n",
    "# file_path = 'yahoo_finance_AAPL_historical.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Remove rows that have 'Dividend' in any column\n",
    "# df = df[~df.apply(lambda row: row.astype(str).str.contains('Dividend').any(), axis=1)]\n",
    "\n",
    "# # Process each row\n",
    "# def clean_row(row):\n",
    "#     # Fix date format: \"Oct 31, 2025\" -> \"Oct-31-2025\"\n",
    "#     try:\n",
    "#         row[0] = datetime.strptime(row[0], \"%b %d, %Y\").strftime(\"%b-%d-%Y\")\n",
    "#     except Exception:\n",
    "#         pass  # Skip rows where date is not in expected format\n",
    "    \n",
    "#     # Clean volume: \"37,689,790\" -> 37689790 (on last col)\n",
    "#     try:\n",
    "#         volume_str = str(row[-1]).replace(\",\", \"\")\n",
    "#         if volume_str.isdigit():\n",
    "#             row[-1] = int(volume_str)\n",
    "#     except Exception:\n",
    "#         pass  # If conversion fails, leave as is\n",
    "#     return row\n",
    "\n",
    "# df = df.apply(clean_row, axis=1)\n",
    "\n",
    "# # Save the cleaned DataFrame back to the CSV file\n",
    "# df.to_csv(file_path, index=False)\n",
    "# print(\"Cleaned data saved to\", file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "39e5d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data reading\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv('yahoo_finance_AAPL_historical.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3229e961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date <class 'str'>\n",
      "Open <class 'str'>\n",
      "High <class 'numpy.float64'>\n",
      "Low <class 'numpy.float64'>\n",
      "Close <class 'numpy.float64'>\n",
      "Adj_Close <class 'numpy.float64'>\n",
      "Volume <class 'str'>\n",
      "Label <class 'str'>\n",
      "Predicted_Label <class 'str'>\n",
      "Date               0\n",
      "Open               0\n",
      "High               0\n",
      "Low                0\n",
      "Close              0\n",
      "Adj_Close          0\n",
      "Volume             5\n",
      "Label              0\n",
      "Predicted_Label    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113245/4144968491.py:6: FutureWarning: DataFrame.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
      "  df.interpolate(method='linear', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# checking data types\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col].iloc[0]))\n",
    "\n",
    "# null value handling\n",
    "df.interpolate(method='linear', inplace=True)\n",
    "null_counts = df.isnull().sum()\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "72507083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Labelling data\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# csv_path = \"/home/hp/Study/Code/AI/Equity_time_series_high_low_detector/yahoo_finance_AAPL_historical.csv\"\n",
    "# df = pd.read_csv(csv_path)\n",
    "\n",
    "# df['Date'] = pd.to_datetime(df['Date'], format='%b-%d-%Y', errors='coerce')\n",
    "\n",
    "# with open(\"reversal.txt\", \"r\") as f:\n",
    "#     reversal_dates = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# df['Label'] = 'N'\n",
    "\n",
    "# df.loc[df['Date'].dt.strftime('%Y-%m-%d').isin(reversal_dates), 'Label'] = 'R'\n",
    "\n",
    "# df.to_csv(csv_path, index=False)\n",
    "\n",
    "# print(\"✅ Labels updated successfully and date format normalized.\")\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "a0f10dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved chart.csv successfully with columns: time, Open, High, Low, Close for date range 1981-06-27 to 2000-08-25\n"
     ]
    }
   ],
   "source": [
    "# making chart.csv for graph plotting\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df already exists and has 'Date', 'Open', 'High', 'Low', 'Close' columns\n",
    "path = \"/home/hp/Study/Code/AI/Equity_time_series_high_low_detector/yahoo_finance_AAPL_historical.csv\"\n",
    "df = pd.read_csv(path)\n",
    "start_date = \"1981-06-27\"\n",
    "end_date = \"2000-08-25\"\n",
    "\n",
    "df_filtered = df.copy()\n",
    "\n",
    "# Ensure correct data types\n",
    "df_filtered[\"Date\"] = pd.to_datetime(df_filtered[\"Date\"], errors=\"coerce\")\n",
    "numeric_cols = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "df_filtered[numeric_cols] = df_filtered[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Drop rows with missing or invalid data\n",
    "df_filtered = df_filtered.dropna(subset=[\"Date\"] + numeric_cols)\n",
    "\n",
    "# Filter by date range\n",
    "df_filtered = df_filtered[(df_filtered[\"Date\"] >= start_date) & (df_filtered[\"Date\"] <= end_date)]\n",
    "\n",
    "# Prepare and save data\n",
    "df_to_save = df_filtered[[\"Date\", \"Open\", \"High\", \"Low\", \"Close\",\"Label\",\"Predicted_Label\"]].copy()\n",
    "df_to_save[\"Date\"] = df_to_save[\"Date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "df_to_save.rename(columns={\"Date\": \"time\"}, inplace=True)\n",
    "\n",
    "df_to_save.to_csv(\"chart.csv\", index=False)\n",
    "\n",
    "print(f\"✅ Saved chart.csv successfully with columns: time, Open, High, Low, Close for date range {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5387c512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 1.127593\n",
      "Epoch [2/10] Loss: 1.127177\n",
      "Epoch [3/10] Loss: 1.127255\n",
      "Epoch [4/10] Loss: 1.127333\n",
      "Epoch [5/10] Loss: 1.127410\n",
      "Epoch [6/10] Loss: 1.127255\n",
      "Epoch [7/10] Loss: 1.127177\n",
      "Epoch [8/10] Loss: 1.127333\n",
      "Epoch [9/10] Loss: 1.127177\n",
      "Epoch [10/10] Loss: 1.127410\n",
      "\n",
      "Training (LSTM + XGBoost) Metrics:\n",
      "Accuracy: 0.9593 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000\n",
      "Confusion Matrix:\n",
      " [[8586    0]\n",
      " [ 364    0]]\n",
      "\n",
      "Validation (LSTM + XGBoost) Metrics:\n",
      "Accuracy: 0.9781 | Precision: 0.0000 | Recall: 0.0000 | F1: 0.0000\n",
      "Confusion Matrix:\n",
      " [[2189    0]\n",
      " [  49    0]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (11188) does not match length of index (11194)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 120\u001b[0m\n\u001b[1;32m    117\u001b[0m pred_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y_all_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    119\u001b[0m df_original \u001b[38;5;241m=\u001b[39m df_original\u001b[38;5;241m.\u001b[39mloc[SEQ_LEN:]\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 120\u001b[0m \u001b[43mdf_original\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPredicted_Label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m pred_labels[:\u001b[38;5;28mlen\u001b[39m(df_original)]\n\u001b[1;32m    121\u001b[0m df_original\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4316\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4315\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[0;32m-> 4316\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:4529\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4521\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4522\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4529\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4531\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   4532\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m   4533\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   4534\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4535\u001b[0m     ):\n\u001b[1;32m   4536\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4537\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:5273\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m   5272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5273\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5274\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   5275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5276\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[1;32m   5277\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5280\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[1;32m   5281\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[0;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (11188) does not match length of index (11194)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score, precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "\n",
    "file_path = \"/home/hp/Study/Code/AI/Equity_time_series_high_low_detector/yahoo_finance_AAPL_historical.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df_original = df.copy()\n",
    "\n",
    "df['Label'] = df['Label'].map({'N': 0, 'R': 1}).fillna(0).astype(int)\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df = df.dropna(subset=numeric_cols).reset_index(drop=True)\n",
    "\n",
    "X = df[numeric_cols].values\n",
    "y = df['Label'].values\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "SEQ_LEN = 20\n",
    "def create_sequences(data, labels, seq_len):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - seq_len):\n",
    "        X_seq.append(data[i:i+seq_len])\n",
    "        y_seq.append(labels[i+seq_len])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "X_seq, y_seq = create_sequences(X_scaled, y, SEQ_LEN)\n",
    "split_idx = int(len(X_seq) * 0.8)\n",
    "X_train, y_train = X_seq[:split_idx], y_seq[:split_idx]\n",
    "X_val, y_val = X_seq[split_idx:], y_seq[split_idx:]\n",
    "\n",
    "class LSTMFeatureExtractor(nn.Module):\n",
    "    def __init__(self, input_dim=6, hidden_dim=64, num_layers=3, feature_dim=64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, feature_dim)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = torch.relu(self.fc(out))\n",
    "        return out\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lstm_model = LSTMFeatureExtractor().to(device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0], device=device))\n",
    "optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TimeSeriesDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(TimeSeriesDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "\n",
    "EPOCHS = 10\n",
    "for epoch in range(EPOCHS):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device).unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        feats = lstm_model(X_batch)\n",
    "        pred = torch.sigmoid(torch.mean(feats, dim=1, keepdim=True))\n",
    "        loss = criterion(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] Loss: {total_loss/len(train_loader):.6f}\")\n",
    "\n",
    "lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    X_lstm_train = lstm_model(torch.tensor(X_train, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    X_lstm_val = lstm_model(torch.tensor(X_val, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    X_lstm_all = lstm_model(torch.tensor(X_seq, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=8,\n",
    "    learning_rate=0.02,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=(np.sum(y_train == 0) / np.sum(y_train == 1)),\n",
    "    tree_method=\"hist\",\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_lstm_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_lstm_train)\n",
    "y_pred_val = model.predict(X_lstm_val)\n",
    "\n",
    "def print_metrics(y_true, y_pred, name):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\n{name} Metrics:\")\n",
    "    print(f\"Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print_metrics(y_train, y_pred_train, \"Training (LSTM + XGBoost)\")\n",
    "print_metrics(y_val, y_pred_val, \"Validation (LSTM + XGBoost)\")\n",
    "\n",
    "y_all_pred = model.predict(X_lstm_all)\n",
    "pred_labels = np.where(y_all_pred == 1, 'R', 'N')\n",
    "\n",
    "df_original = df_original.loc[SEQ_LEN:].copy().reset_index(drop=True)\n",
    "df_original['Predicted_Label'] = pred_labels[:len(df_original)]\n",
    "df_original.to_csv(file_path, index=False)\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Equity",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
