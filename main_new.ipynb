{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":114135,"databundleVersionId":13695648,"sourceType":"competition"},{"sourceId":13179788,"sourceType":"datasetVersion","datasetId":8352088}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"d56a3f60","cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:10:23.674800Z","iopub.execute_input":"2025-10-26T05:10:23.675166Z","iopub.status.idle":"2025-10-26T05:10:23.683066Z","shell.execute_reply.started":"2025-10-26T05:10:23.675111Z","shell.execute_reply":"2025-10-26T05:10:23.682235Z"}},"outputs":[],"execution_count":1},{"id":"4fbf02d9","cell_type":"code","source":"import csv\nimport os\n\n# Input and output file paths\ninput_train_file = \"/kaggle/input/detecting-reversal-points-in-us-equities/competition_data/train.csv\"\noutput_train_file = \"modified_train.csv\"\ninput_test_file = \"/kaggle/input/detecting-reversal-points-in-us-equities/competition_data/test.csv\"\noutput_test_file = \"modified_test.csv\"\n\n# Function to process the CSV\ndef process_csv(input_path, output_path):\n    with open(input_path, 'r', newline='') as infile, open(output_path, 'w', newline='') as outfile:\n        reader = csv.reader(infile)\n        writer = csv.writer(outfile)\n        \n        # Read the header row\n        header = next(reader)\n        \n        # Identify indices of columns with specific prefixes\n        happens_within_indices = [i for i, col in enumerate(header) if col.startswith('happens_within')]\n        crossing_above_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_above')]\n        crossing_below_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_below')]\n        trending_down_indices = [i for i, col in enumerate(header) if col.startswith('trending_down_and_above')]\n        trending_up_indices = [i for i, col in enumerate(header) if col.startswith('trending_up_and_below')]\n        troughs_indices = [i for i, col in enumerate(header) if col.startswith('troughs')]\n        peaks_indices = [i for i, col in enumerate(header) if col.startswith('peaks')]\n        \n        # Keep columns that are not any of the specified prefixes\n        other_indices = [i for i, col in enumerate(header) if not (\n            col.startswith('happens_within') or \n            col.startswith('occurs_within') or \n            col.startswith('cross_threshold_from_above') or \n            col.startswith('cross_threshold_from_below') or\n            col.startswith('trending_down_and_above') or\n            col.startswith('trending_up_and_below') or\n            col.startswith('troughs') or\n            col.startswith('zone') or\n            col.startswith('peaks')\n        )]\n        \n        # Create new header: keep other columns, add new columns for counts\n        new_header = [header[i] for i in other_indices] + [\n            'happens_within',  \n            'cross_threshold_from_above', \n            'cross_threshold_from_below',\n            'trending_down_and_above',\n            'trending_up_and_below',\n            'troughs',\n            'peaks'\n        ]\n        writer.writerow(new_header)\n        \n        # Process each data row\n        for row in reader:\n            # Count \"True\" values in each set of columns\n            happens_within_count = sum(1 for i in happens_within_indices if row[i] == 'True')\n            crossing_above_count = sum(1 for i in crossing_above_indices if row[i] == 'True')\n            crossing_below_count = sum(1 for i in crossing_below_indices if row[i] == 'True')\n            trending_down_count = sum(1 for i in trending_down_indices if row[i] == 'True')\n            trending_up_count = sum(1 for i in trending_up_indices if row[i] == 'True')\n            troughs_count = sum(1 for i in troughs_indices if row[i] == 'True')\n            peaks_count = sum(1 for i in peaks_indices if row[i] == 'True')\n            \n            # Create new row: keep other columns, add counted columns\n            new_row = [row[i] for i in other_indices] + [\n                str(happens_within_count), \n                str(crossing_above_count), \n                str(crossing_below_count),\n                str(trending_down_count),\n                str(trending_up_count),\n                str(troughs_count),\n                str(peaks_count)\n            ]\n            writer.writerow(new_row)\n\n# Execute the processing\nprocess_csv(input_train_file, output_train_file)\nprint(f\"Processed CSV saved as {output_train_file}\")\nprocess_csv(input_test_file, output_test_file)\nprint(f\"Processed CSV saved as {output_test_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:10:26.260460Z","iopub.execute_input":"2025-10-26T05:10:26.260780Z","iopub.status.idle":"2025-10-26T05:11:01.790695Z","shell.execute_reply.started":"2025-10-26T05:10:26.260756Z","shell.execute_reply":"2025-10-26T05:11:01.789571Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3835054508.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mprocess_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_train_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mprocess_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_test_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_test_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processed CSV saved as {output_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'output_file' is not defined"],"ename":"NameError","evalue":"name 'output_file' is not defined","output_type":"error"}],"execution_count":2},{"id":"5afe0f0e","cell_type":"code","source":"# import csv\n# import os\n\n# # Input and output file paths\n# input_file = \"/home/hp/Study/Code/AI/Equity_time_series_high_low_detector/detecting-reversal-points-in-us-equities/competition_data/test.csv\"\n# output_file = \"modified_test.csv\"\n\n# # Function to process the CSV\n# def process_csv(input_path, output_path):\n#     with open(input_path, 'r', newline='') as infile, open(output_path, 'w', newline='') as outfile:\n#         reader = csv.reader(infile)\n#         writer = csv.writer(outfile)\n        \n#         # Read the header row\n#         header = next(reader)\n        \n#         # Identify indices of columns with specific prefixes\n#         happens_within_indices = [i for i, col in enumerate(header) if col.startswith('happens_within')]\n#         crossing_above_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_above')]\n#         crossing_below_indices = [i for i, col in enumerate(header) if col.startswith('cross_threshold_from_below')]\n#         trending_down_indices = [i for i, col in enumerate(header) if col.startswith('trending_down_and_above')]\n#         trending_up_indices = [i for i, col in enumerate(header) if col.startswith('trending_up_and_below')]\n#         troughs_indices = [i for i, col in enumerate(header) if col.startswith('troughs')]\n#         peaks_indices = [i for i, col in enumerate(header) if col.startswith('peaks')]\n        \n#         # Keep columns that are not any of the specified prefixes\n#         other_indices = [i for i, col in enumerate(header) if not (\n#             col.startswith('happens_within') or \n#             col.startswith('occurs_within') or \n#             col.startswith('cross_threshold_from_above') or \n#             col.startswith('cross_threshold_from_below') or\n#             col.startswith('trending_down_and_above') or\n#             col.startswith('trending_up_and_below') or\n#             col.startswith('troughs') or\n#             col.startswith('zone') or\n#             col.startswith('peaks')\n#         )]\n        \n#         # Create new header: keep other columns, add new columns for counts\n#         new_header = [header[i] for i in other_indices] + [\n#             'happens_within', \n#             'cross_threshold_from_above', \n#             'cross_threshold_from_below',\n#             'trending_down_and_above',\n#             'trending_up_and_below',\n#             'troughs',\n#             'peaks'\n#         ]\n#         writer.writerow(new_header)\n        \n#         # Process each data row\n#         for row in reader:\n#             # Count \"True\" values in each set of columns\n#             happens_within_count = sum(1 for i in happens_within_indices if row[i] == 'True')\n#             crossing_above_count = sum(1 for i in crossing_above_indices if row[i] == 'True')\n#             crossing_below_count = sum(1 for i in crossing_below_indices if row[i] == 'True')\n#             trending_down_count = sum(1 for i in trending_down_indices if row[i] == 'True')\n#             trending_up_count = sum(1 for i in trending_up_indices if row[i] == 'True')\n#             troughs_count = sum(1 for i in troughs_indices if row[i] == 'True')\n#             peaks_count = sum(1 for i in peaks_indices if row[i] == 'True')\n            \n#             # Create new row: keep other columns, add counted columns\n#             new_row = [row[i] for i in other_indices] + [\n#                 str(happens_within_count), \n#                 str(crossing_above_count), \n#                 str(crossing_below_count),\n#                 str(trending_down_count),\n#                 str(trending_up_count),\n#                 str(troughs_count),\n#                 str(peaks_count)\n#             ]\n#             writer.writerow(new_row)\n\n# # Execute the processing\n# process_csv(input_file, output_file)\n# print(f\"Processed CSV saved as {output_file}\")","metadata":{},"outputs":[],"execution_count":4},{"id":"5a2ef0fc","cell_type":"code","source":"import pandas as pd\n\ndef sort_csv_by_column(file_path, column_name):\n    # Read CSV\n    df = pd.read_csv(file_path)\n\n    # Sort by column 't'\n    df_sorted = df.sort_values(by=column_name)\n\n    # Write back to same file (overwrite)\n    df_sorted.to_csv(file_path, index=False)\n\n    print(f\"File '{file_path}' has been sorted by column '{column_name}' and overwritten.\")\n\nsort_csv_by_column('modified_train.csv', 't')\nsort_csv_by_column('modified_test.csv', 't')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:11:16.413591Z","iopub.execute_input":"2025-10-26T05:11:16.413935Z","iopub.status.idle":"2025-10-26T05:11:16.807986Z","shell.execute_reply.started":"2025-10-26T05:11:16.413911Z","shell.execute_reply":"2025-10-26T05:11:16.806997Z"}},"outputs":[{"name":"stdout","text":"File 'modified_train.csv' has been sorted by column 't' and overwritten.\nFile 'modified_test.csv' has been sorted by column 't' and overwritten.\n","output_type":"stream"}],"execution_count":3},{"id":"8fe5109f","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score, matthews_corrcoef\nfrom sklearn.utils.class_weight import compute_class_weight\nimport xgboost as xgb\nimport time\n\n# File path for train\ninput_train_file = \"modified_train.csv\"\ninput_test_file = \"modified_test.csv\"\n\n# Load the CSV into a Pandas DataFrame\ndf_train = pd.read_csv(input_train_file)\ndf_test = pd.read_csv(input_test_file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:11:26.238457Z","iopub.execute_input":"2025-10-26T05:11:26.238854Z","iopub.status.idle":"2025-10-26T05:11:27.260080Z","shell.execute_reply.started":"2025-10-26T05:11:26.238826Z","shell.execute_reply":"2025-10-26T05:11:27.259278Z"}},"outputs":[],"execution_count":4},{"id":"742a2554","cell_type":"code","source":"df_train['class_label'] = df_train['class_label'].replace({\n    'HH': 'H',\n    'LH': 'H',\n    'LL': 'L',\n    'HL': 'L'\n}).where(df_train['class_label'].isin(['HH', 'LH', 'LL', 'HL']), other='N')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:11:46.253885Z","iopub.execute_input":"2025-10-26T05:11:46.254687Z","iopub.status.idle":"2025-10-26T05:11:46.268682Z","shell.execute_reply.started":"2025-10-26T05:11:46.254650Z","shell.execute_reply":"2025-10-26T05:11:46.267718Z"}},"outputs":[],"execution_count":5},{"id":"671ec21e","cell_type":"code","source":"import pandas as pd\n\nprint(\"Descriptive statistics:\")\nprint(df_train.describe(include='all'))   \n\nprint(\"Info & null counts:\")\nprint(df_train.info())\nprint(\"\\nMissing values per column:\")\nprint(df_train.isnull().sum())\n\nprint(\"Unique values per column:\")\nfor col in df_train.columns:\n    print(f\"{col}: {df_train[col].nunique()} unique values\")\n\nprint(\"Sample categories per column:\")\nfor col in df_train.select_dtypes(include='object').columns:\n    print(f\"{col}: {df_train[col].unique()[:10]}\")  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:13:17.889472Z","iopub.execute_input":"2025-10-26T05:13:17.889746Z","iopub.status.idle":"2025-10-26T05:13:17.951653Z","shell.execute_reply.started":"2025-10-26T05:13:17.889728Z","shell.execute_reply":"2025-10-26T05:13:17.950376Z"}},"outputs":[{"name":"stdout","text":"Descriptive statistics:\n           train_id    ticker_id           t     momentum        ratio  \\\ncount   1932.000000  1932.000000        1932  1932.000000  1932.000000   \nunique          NaN          NaN         460          NaN          NaN   \ntop             NaN          NaN  2024-01-12          NaN          NaN   \nfreq            NaN          NaN           6          NaN          NaN   \nmean     965.500000     3.500000         NaN    99.992433   100.003231   \nstd      557.864679     1.708267         NaN     0.766347     1.037714   \nmin        0.000000     1.000000         NaN    96.087990    95.327515   \n25%      482.750000     2.000000         NaN    99.573017    99.402484   \n50%      965.500000     3.500000         NaN    99.985227    99.981954   \n75%     1448.250000     5.000000         NaN   100.444782   100.634551   \nmax     1931.000000     6.000000         NaN   102.465023   103.870229   \n\n        sm_momentum     sm_ratio class_label  happens_within  \\\ncount   1932.000000  1932.000000        1932     1932.000000   \nunique          NaN          NaN           3             NaN   \ntop             NaN          NaN           N             NaN   \nfreq            NaN          NaN        1820             NaN   \nmean      99.981216    99.969029         NaN      126.613354   \nstd        0.740832     0.488027         NaN      175.280298   \nmin       97.709038    98.494252         NaN        2.000000   \n25%       99.519300    99.640337         NaN        9.000000   \n50%       99.964999    99.980883         NaN       65.500000   \n75%      100.434216   100.265497         NaN      190.000000   \nmax      102.841981   101.501212         NaN     1983.000000   \n\n        cross_threshold_from_above  cross_threshold_from_below  \\\ncount                  1932.000000                 1932.000000   \nunique                         NaN                         NaN   \ntop                            NaN                         NaN   \nfreq                           NaN                         NaN   \nmean                      0.182712                    0.192029   \nstd                       0.403572                    0.401807   \nmin                       0.000000                    0.000000   \n25%                       0.000000                    0.000000   \n50%                       0.000000                    0.000000   \n75%                       0.000000                    0.000000   \nmax                       2.000000                    2.000000   \n\n        trending_down_and_above  trending_up_and_below      troughs  \\\ncount               1932.000000            1932.000000  1932.000000   \nunique                      NaN                    NaN          NaN   \ntop                         NaN                    NaN          NaN   \nfreq                        NaN                    NaN          NaN   \nmean                   0.256729               0.276398     0.143892   \nstd                    0.648730               0.652607     0.598634   \nmin                    0.000000               0.000000     0.000000   \n25%                    0.000000               0.000000     0.000000   \n50%                    0.000000               0.000000     0.000000   \n75%                    0.000000               0.000000     0.000000   \nmax                    5.000000               4.000000     5.000000   \n\n              peaks  \ncount   1932.000000  \nunique          NaN  \ntop             NaN  \nfreq            NaN  \nmean       0.125776  \nstd        0.545418  \nmin        0.000000  \n25%        0.000000  \n50%        0.000000  \n75%        0.000000  \nmax        5.000000  \nInfo & null counts:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1932 entries, 0 to 1931\nData columns (total 15 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   train_id                    1932 non-null   int64  \n 1   ticker_id                   1932 non-null   int64  \n 2   t                           1932 non-null   object \n 3   momentum                    1932 non-null   float64\n 4   ratio                       1932 non-null   float64\n 5   sm_momentum                 1932 non-null   float64\n 6   sm_ratio                    1932 non-null   float64\n 7   class_label                 1932 non-null   object \n 8   happens_within              1932 non-null   int64  \n 9   cross_threshold_from_above  1932 non-null   int64  \n 10  cross_threshold_from_below  1932 non-null   int64  \n 11  trending_down_and_above     1932 non-null   int64  \n 12  trending_up_and_below       1932 non-null   int64  \n 13  troughs                     1932 non-null   int64  \n 14  peaks                       1932 non-null   int64  \ndtypes: float64(4), int64(9), object(2)\nmemory usage: 226.5+ KB\nNone\n\nMissing values per column:\ntrain_id                      0\nticker_id                     0\nt                             0\nmomentum                      0\nratio                         0\nsm_momentum                   0\nsm_ratio                      0\nclass_label                   0\nhappens_within                0\ncross_threshold_from_above    0\ncross_threshold_from_below    0\ntrending_down_and_above       0\ntrending_up_and_below         0\ntroughs                       0\npeaks                         0\ndtype: int64\nUnique values per column:\ntrain_id: 1932 unique values\nticker_id: 6 unique values\nt: 460 unique values\nmomentum: 1931 unique values\nratio: 1930 unique values\nsm_momentum: 1932 unique values\nsm_ratio: 1708 unique values\nclass_label: 3 unique values\nhappens_within: 410 unique values\ncross_threshold_from_above: 3 unique values\ncross_threshold_from_below: 3 unique values\ntrending_down_and_above: 6 unique values\ntrending_up_and_below: 5 unique values\ntroughs: 6 unique values\npeaks: 6 unique values\nSample categories per column:\nt: ['2023-04-03' '2023-04-04' '2023-04-05' '2023-04-06' '2023-04-10'\n '2023-04-11' '2023-04-12' '2023-04-13' '2023-04-14' '2023-04-17']\nclass_label: ['N' 'H' 'L']\n","output_type":"stream"}],"execution_count":6},{"id":"f6cba4a3-3a58-410e-af9c-43bb67edde5f","cell_type":"code","source":"import pandas as pd\n\nprint(\"Descriptive statistics:\")\nprint(df_test.describe(include='all'))   \n\nprint(\"Info & null counts:\")\nprint(df_test.info())\nprint(\"\\nMissing values per column:\")\nprint(df_test.isnull().sum())\n\nprint(\"Unique values per column:\")\nfor col in df_test.columns:\n    print(f\"{col}: {df_test[col].nunique()} unique values\")\n\nprint(\"Sample categories per column:\")\nfor col in df_test.select_dtypes(include='object').columns:\n    print(f\"{col}: {df_test[col].unique()[:10]}\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:14:55.807449Z","iopub.execute_input":"2025-10-26T05:14:55.808045Z","iopub.status.idle":"2025-10-26T05:14:55.851167Z","shell.execute_reply.started":"2025-10-26T05:14:55.808016Z","shell.execute_reply":"2025-10-26T05:14:55.850358Z"}},"outputs":[{"name":"stdout","text":"Descriptive statistics:\n                id   ticker_id           t    momentum       ratio  \\\ncount   828.000000  828.000000         828  828.000000  828.000000   \nunique         NaN         NaN         410         NaN         NaN   \ntop            NaN         NaN  2023-06-30         NaN         NaN   \nfreq           NaN         NaN           5         NaN         NaN   \nmean    413.500000    3.500000         NaN   99.978878   99.997910   \nstd     239.167305    1.708857         NaN    0.768823    1.088092   \nmin       0.000000    1.000000         NaN   96.513556   95.641552   \n25%     206.750000    2.000000         NaN   99.500658   99.405569   \n50%     413.500000    3.500000         NaN   99.975093  100.028616   \n75%     620.250000    5.000000         NaN  100.484466  100.646528   \nmax     827.000000    6.000000         NaN  102.504383  103.894367   \n\n        sm_momentum    sm_ratio  happens_within  cross_threshold_from_above  \\\ncount    828.000000  828.000000      828.000000                  828.000000   \nunique          NaN         NaN             NaN                         NaN   \ntop             NaN         NaN             NaN                         NaN   \nfreq            NaN         NaN             NaN                         NaN   \nmean     100.037272   99.984686      122.561594                    0.199275   \nstd        0.716037    0.492452      153.678931                    0.417454   \nmin       97.939548   98.650629        2.000000                    0.000000   \n25%       99.600110   99.645702       10.000000                    0.000000   \n50%      100.016653   99.978166       71.000000                    0.000000   \n75%      100.459477  100.280448      183.000000                    0.000000   \nmax      102.542293  101.309449     1006.000000                    2.000000   \n\n        cross_threshold_from_below  trending_down_and_above  \\\ncount                   828.000000               828.000000   \nunique                         NaN                      NaN   \ntop                            NaN                      NaN   \nfreq                           NaN                      NaN   \nmean                      0.176329                 0.258454   \nstd                       0.387620                 0.660412   \nmin                       0.000000                 0.000000   \n25%                       0.000000                 0.000000   \n50%                       0.000000                 0.000000   \n75%                       0.000000                 0.000000   \nmax                       2.000000                 4.000000   \n\n        trending_up_and_below     troughs       peaks  \ncount              828.000000  828.000000  828.000000  \nunique                    NaN         NaN         NaN  \ntop                       NaN         NaN         NaN  \nfreq                      NaN         NaN         NaN  \nmean                 0.264493    0.120773    0.154589  \nstd                  0.626960    0.527276    0.622252  \nmin                  0.000000    0.000000    0.000000  \n25%                  0.000000    0.000000    0.000000  \n50%                  0.000000    0.000000    0.000000  \n75%                  0.000000    0.000000    0.000000  \nmax                  4.000000    5.000000    5.000000  \nInfo & null counts:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 828 entries, 0 to 827\nData columns (total 14 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   id                          828 non-null    int64  \n 1   ticker_id                   828 non-null    int64  \n 2   t                           828 non-null    object \n 3   momentum                    828 non-null    float64\n 4   ratio                       828 non-null    float64\n 5   sm_momentum                 828 non-null    float64\n 6   sm_ratio                    828 non-null    float64\n 7   happens_within              828 non-null    int64  \n 8   cross_threshold_from_above  828 non-null    int64  \n 9   cross_threshold_from_below  828 non-null    int64  \n 10  trending_down_and_above     828 non-null    int64  \n 11  trending_up_and_below       828 non-null    int64  \n 12  troughs                     828 non-null    int64  \n 13  peaks                       828 non-null    int64  \ndtypes: float64(4), int64(9), object(1)\nmemory usage: 90.7+ KB\nNone\n\nMissing values per column:\nid                            0\nticker_id                     0\nt                             0\nmomentum                      0\nratio                         0\nsm_momentum                   0\nsm_ratio                      0\nhappens_within                0\ncross_threshold_from_above    0\ncross_threshold_from_below    0\ntrending_down_and_above       0\ntrending_up_and_below         0\ntroughs                       0\npeaks                         0\ndtype: int64\nUnique values per column:\nid: 828 unique values\nticker_id: 6 unique values\nt: 410 unique values\nmomentum: 828 unique values\nratio: 828 unique values\nsm_momentum: 828 unique values\nsm_ratio: 791 unique values\nhappens_within: 285 unique values\ncross_threshold_from_above: 3 unique values\ncross_threshold_from_below: 3 unique values\ntrending_down_and_above: 5 unique values\ntrending_up_and_below: 5 unique values\ntroughs: 6 unique values\npeaks: 6 unique values\nSample categories per column:\nt: ['2023-04-03' '2023-04-04' '2023-04-05' '2023-04-06' '2023-04-10'\n '2023-04-11' '2023-04-12' '2023-04-13' '2023-04-14' '2023-04-17']\n","output_type":"stream"}],"execution_count":7},{"id":"9c0a3510","cell_type":"code","source":"df_train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:15:37.699253Z","iopub.execute_input":"2025-10-26T05:15:37.699952Z","iopub.status.idle":"2025-10-26T05:15:37.721306Z","shell.execute_reply.started":"2025-10-26T05:15:37.699922Z","shell.execute_reply":"2025-10-26T05:15:37.720357Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   train_id  ticker_id           t    momentum       ratio  sm_momentum  \\\n0       775          6  2023-04-03   99.999622  100.153223    99.403022   \n1       410          1  2023-04-03   99.131989  100.162810    99.942529   \n2      1363          4  2023-04-03   99.657066  100.054925   100.764058   \n3       490          2  2023-04-03  100.065645  100.174210    99.003064   \n4       364          3  2023-04-04  100.859582  101.387954   102.188969   \n\n     sm_ratio class_label  happens_within  cross_threshold_from_above  \\\n0   99.570105           N              20                           0   \n1   99.922969           N              20                           0   \n2   99.579857           H              20                           0   \n3  100.347891           N              20                           0   \n4   99.440955           N               2                           0   \n\n   cross_threshold_from_below  trending_down_and_above  trending_up_and_below  \\\n0                           0                        0                      0   \n1                           0                        0                      0   \n2                           0                        0                      0   \n3                           0                        0                      0   \n4                           0                        0                      0   \n\n   troughs  peaks  \n0        0      0  \n1        0      0  \n2        0      0  \n3        0      0  \n4        0      0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>train_id</th>\n      <th>ticker_id</th>\n      <th>t</th>\n      <th>momentum</th>\n      <th>ratio</th>\n      <th>sm_momentum</th>\n      <th>sm_ratio</th>\n      <th>class_label</th>\n      <th>happens_within</th>\n      <th>cross_threshold_from_above</th>\n      <th>cross_threshold_from_below</th>\n      <th>trending_down_and_above</th>\n      <th>trending_up_and_below</th>\n      <th>troughs</th>\n      <th>peaks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>775</td>\n      <td>6</td>\n      <td>2023-04-03</td>\n      <td>99.999622</td>\n      <td>100.153223</td>\n      <td>99.403022</td>\n      <td>99.570105</td>\n      <td>N</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>410</td>\n      <td>1</td>\n      <td>2023-04-03</td>\n      <td>99.131989</td>\n      <td>100.162810</td>\n      <td>99.942529</td>\n      <td>99.922969</td>\n      <td>N</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1363</td>\n      <td>4</td>\n      <td>2023-04-03</td>\n      <td>99.657066</td>\n      <td>100.054925</td>\n      <td>100.764058</td>\n      <td>99.579857</td>\n      <td>H</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>490</td>\n      <td>2</td>\n      <td>2023-04-03</td>\n      <td>100.065645</td>\n      <td>100.174210</td>\n      <td>99.003064</td>\n      <td>100.347891</td>\n      <td>N</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>364</td>\n      <td>3</td>\n      <td>2023-04-04</td>\n      <td>100.859582</td>\n      <td>101.387954</td>\n      <td>102.188969</td>\n      <td>99.440955</td>\n      <td>N</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"id":"de75bb5b-f6a8-4d16-a7e2-92d5a70dd848","cell_type":"code","source":"df_test.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-26T05:15:49.551875Z","iopub.execute_input":"2025-10-26T05:15:49.552247Z","iopub.status.idle":"2025-10-26T05:15:49.565418Z","shell.execute_reply.started":"2025-10-26T05:15:49.552222Z","shell.execute_reply":"2025-10-26T05:15:49.564421Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"    id  ticker_id           t    momentum       ratio  sm_momentum  \\\n0  199          5  2023-04-03   98.028059  100.167714    99.010811   \n1  753          3  2023-04-03  101.724160  101.814890   102.358914   \n2  316          5  2023-04-04   97.921661   99.677934    99.071748   \n3  313          1  2023-04-04   99.166349  100.040334    99.746758   \n4  536          4  2023-04-05   99.408127   99.734170   100.738871   \n\n     sm_ratio  happens_within  cross_threshold_from_above  \\\n0  100.347891              20                           0   \n1   99.318084              20                           0   \n2  100.292253               2                           0   \n3   99.826205               2                           0   \n4   99.712689             104                           0   \n\n   cross_threshold_from_below  trending_down_and_above  trending_up_and_below  \\\n0                           0                        0                      0   \n1                           0                        0                      0   \n2                           0                        0                      0   \n3                           0                        0                      0   \n4                           0                        0                      0   \n\n   troughs  peaks  \n0        0      0  \n1        0      0  \n2        0      0  \n3        0      0  \n4        0      2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>ticker_id</th>\n      <th>t</th>\n      <th>momentum</th>\n      <th>ratio</th>\n      <th>sm_momentum</th>\n      <th>sm_ratio</th>\n      <th>happens_within</th>\n      <th>cross_threshold_from_above</th>\n      <th>cross_threshold_from_below</th>\n      <th>trending_down_and_above</th>\n      <th>trending_up_and_below</th>\n      <th>troughs</th>\n      <th>peaks</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>199</td>\n      <td>5</td>\n      <td>2023-04-03</td>\n      <td>98.028059</td>\n      <td>100.167714</td>\n      <td>99.010811</td>\n      <td>100.347891</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>753</td>\n      <td>3</td>\n      <td>2023-04-03</td>\n      <td>101.724160</td>\n      <td>101.814890</td>\n      <td>102.358914</td>\n      <td>99.318084</td>\n      <td>20</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>316</td>\n      <td>5</td>\n      <td>2023-04-04</td>\n      <td>97.921661</td>\n      <td>99.677934</td>\n      <td>99.071748</td>\n      <td>100.292253</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>313</td>\n      <td>1</td>\n      <td>2023-04-04</td>\n      <td>99.166349</td>\n      <td>100.040334</td>\n      <td>99.746758</td>\n      <td>99.826205</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>536</td>\n      <td>4</td>\n      <td>2023-04-05</td>\n      <td>99.408127</td>\n      <td>99.734170</td>\n      <td>100.738871</td>\n      <td>99.712689</td>\n      <td>104</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"id":"2fd546d2","cell_type":"code","source":"df_001=df_train[df_train['ticker_id']==1]\ndf_002=df_train[df_train['ticker_id']==2]\ndf_003=df_train[df_train['ticker_id']==3]\ndf_004=df_train[df_train['ticker_id']==4]\ndf_005=df_train[df_train['ticker_id']==5]\ndf_006=df_train[df_train['ticker_id']==6]","metadata":{},"outputs":[],"execution_count":10},{"id":"dda19e46","cell_type":"code","source":"except_features = ['train_id', 'ticker_id', 't', 'class_label']\n\ndfs = [df_001, df_002, df_003, df_004, df_005, df_006]\n\nfeature_stats = {}\n\nfor i, df in enumerate(dfs, start=1):\n    cols_to_scale = [col for col in df.columns if col not in except_features]\n    \n    means = df[cols_to_scale].mean()\n    stds = df[cols_to_scale].std(ddof=0)\n    feature_stats[i] = {'mean': means, 'std': stds}\n\n    df[cols_to_scale] = (df[cols_to_scale] - means) / stds\n\n    globals()[f\"df_{i:03}\"] = df","metadata":{},"outputs":[],"execution_count":11},{"id":"883f52d9","cell_type":"code","source":"import plotly.graph_objects as go\nimport pandas as pd\n\nx = df_001['t'].tolist()\ny1 = df_001['ratio'].tolist()\ny2 = df_001['sm_ratio'].tolist()\ny3 = df_001['momentum'].tolist()\ny4 = df_001['sm_momentum'].tolist()\ny5 = df_001['happens_within'].tolist()\ny7 = df_001['cross_threshold_from_above'].tolist()\ny8 = df_001['cross_threshold_from_below'].tolist()\ny9 = df_001['trending_down_and_above'].tolist()\ny10 = df_001['trending_up_and_below'].tolist()\ny11 = df_001['troughs'].tolist()\ny12 = df_001['peaks'].tolist()\nlabel = df_001['class_label'].tolist()\n\nfig = go.Figure()\n\n# Plot all traces\nfig.add_trace(go.Scatter(x=x, y=y1, mode='lines', name='Momentum', line=dict(color='blue')))\nfig.add_trace(go.Scatter(x=x, y=y2, mode='lines', name='Smoothed Momentum', line=dict(color='orange')))\n# fig.add_trace(go.Scatter(x=x, y=y3, mode='lines', name='Ratio', line=dict(color='green')))\n# fig.add_trace(go.Scatter(x=x, y=y4, mode='lines', name='Smoothed Ratio', line=dict(color='purple')))\n# fig.add_trace(go.Scatter(x=x, y=y5, mode='lines', name='Happens Within', line=dict(color='cyan')))\n# fig.add_trace(go.Scatter(x=x, y=y7, mode='lines', name='Cross Threshold From Above', line=dict(color='yellow')))\n# fig.add_trace(go.Scatter(x=x, y=y8, mode='lines', name='Cross Threshold From Below', line=dict(color='lime')))\n# fig.add_trace(go.Scatter(x=x, y=y9, mode='lines', name='Trending Down And Above', line=dict(color='orange')))\n# fig.add_trace(go.Scatter(x=x, y=y10, mode='lines', name='Trending Up And Below', line=dict(color='lightgreen')))\n# fig.add_trace(go.Scatter(x=x, y=y11, mode='lines', name='Troughs', line=dict(color='brown')))\n# fig.add_trace(go.Scatter(x=x, y=y12, mode='lines', name='Peaks', line=dict(color='pink')))\n\n# Plot H / L labels as text markers\nfor xi, yi, lbl in zip(x, y11, label):\n    if lbl == 'H':\n        fig.add_trace(go.Scatter(\n            x=[xi], y=[yi + 0.1],  # small offset above the line\n            mode='text', text=['H'],\n            textfont=dict(color='red', size=12), showlegend=False\n        ))\n    elif lbl == 'L':\n        fig.add_trace(go.Scatter(\n            x=[xi], y=[yi - 0.1],  # small offset below the line\n            mode='text', text=['L'],\n            textfont=dict(color='red', size=12), showlegend=False\n        ))\n\n# Layout - autoscale y-axis\nfig.update_layout(\n    title='Momentum and Smoothed Momentum with H/L Labels',\n    xaxis_title='Time (t)',\n    yaxis_title='Momentum / Ratio / Events',\n    template='plotly_dark',\n    xaxis=dict(rangeslider=dict(visible=True)),  # Scrollable range slider\n    yaxis=dict(autorange=True),                  # Auto-scale y-axis based on all traces\n    hovermode='x unified'\n)\n\nfig.show()\n","metadata":{},"outputs":[{"data":{"application/vnd.plotly.v1+json":{"config":{"plotlyServerURL":"https://plot.ly"},"data":[{"line":{"color":"blue"},"mode":"lines","name":"Momentum","type":"scatter","x":["2023-04-03","2023-04-05","2023-04-06","2023-04-11","2023-04-12","2023-04-13","2023-04-17","2023-04-18","2023-04-19","2023-04-20","2023-04-25","2023-04-26","2023-04-27","2023-04-28","2023-05-01","2023-05-04","2023-05-05","2023-05-08","2023-05-09","2023-05-10","2023-05-12","2023-05-15","2023-05-16","2023-05-17","2023-05-19","2023-05-23","2023-05-24","2023-05-25","2023-05-26","2023-05-30","2023-06-01","2023-06-02","2023-06-06","2023-06-07","2023-06-08","2023-06-15","2023-06-22","2023-06-23","2023-06-26","2023-06-28","2023-06-29","2023-07-05","2023-07-07","2023-07-10","2023-07-11","2023-07-12","2023-07-13","2023-07-14","2023-07-18","2023-07-19","2023-07-20","2023-07-21","2023-07-24","2023-07-25","2023-07-26","2023-07-27","2023-07-31","2023-08-01","2023-08-02","2023-08-03","2023-08-04","2023-08-07","2023-08-10","2023-08-11","2023-08-15","2023-08-16","2023-08-17","2023-08-18","2023-08-21","2023-08-22","2023-08-28","2023-08-30","2023-08-31","2023-09-06","2023-09-07","2023-09-08","2023-09-11","2023-09-12","2023-09-13","2023-09-14","2023-09-15","2023-09-18","2023-09-19","2023-09-20","2023-09-21","2023-09-22","2023-09-25","2023-09-27","2023-09-28","2023-09-29","2023-10-02","2023-10-03","2023-10-04","2023-10-06","2023-10-10","2023-10-11","2023-10-12","2023-10-17","2023-10-18","2023-10-20","2023-10-23","2023-10-24","2023-10-25","2023-10-26","2023-10-27","2023-10-30","2023-10-31","2023-11-01","2023-11-02","2023-11-06","2023-11-08","2023-11-09","2023-11-10","2023-11-13","2023-11-15","2023-11-16","2023-11-17","2023-11-24","2023-11-27","2023-11-28","2023-11-30","2023-12-01","2023-12-05","2023-12-07","2023-12-08","2023-12-12","2023-12-13","2023-12-14","2023-12-15","2023-12-18","2023-12-19","2023-12-20","2023-12-21","2023-12-22","2023-12-26","2023-12-27","2023-12-28","2024-01-08","2024-01-09","2024-01-10","2024-01-11","2024-01-12","2024-01-16","2024-01-17","2024-01-18","2024-01-23","2024-01-24","2024-01-25","2024-01-29","2024-01-31","2024-02-01","2024-02-02","2024-02-05","2024-02-14","2024-02-15","2024-02-16","2024-02-20","2024-02-21","2024-02-22","2024-02-23","2024-02-26","2024-02-28","2024-02-29","2024-03-01","2024-03-05","2024-03-06","2024-03-07","2024-03-08","2024-03-13","2024-03-14","2024-03-15","2024-03-18","2024-03-20","2024-03-22","2024-03-25","2024-03-26","2024-03-27","2024-03-28","2024-04-01","2024-04-02","2024-04-05","2024-04-08","2024-04-09","2024-04-12","2024-04-17","2024-04-18","2024-04-23","2024-04-24","2024-04-26","2024-05-01","2024-05-02","2024-05-06","2024-05-07","2024-05-09","2024-05-10","2024-05-14","2024-05-15","2024-05-16","2024-05-17","2024-05-20","2024-05-21","2024-05-22","2024-05-23","2024-05-28","2024-05-29","2024-05-30","2024-06-04","2024-06-06","2024-06-07","2024-06-10","2024-06-11","2024-06-13","2024-06-14","2024-06-18","2024-06-20","2024-06-24","2024-06-27","2024-06-28","2024-07-01","2024-07-03","2024-07-08","2024-07-09","2024-07-10","2024-07-12","2024-07-15","2024-07-16","2024-07-17","2024-07-18","2024-07-19","2024-07-25","2024-07-29","2024-07-30","2024-07-31","2024-08-01","2024-08-02","2024-08-05","2024-08-07","2024-08-09","2024-08-12","2024-08-13","2024-08-19","2024-08-20","2024-08-22","2024-08-26","2024-08-27","2024-08-28","2024-08-29","2024-08-30","2024-09-03","2024-09-04","2024-09-05","2024-09-10","2024-09-11","2024-09-13","2024-09-16","2024-09-18","2024-09-19","2024-09-20","2024-09-23","2024-09-24","2024-09-27","2024-09-30","2024-10-01","2024-10-02","2024-10-03","2024-10-04","2024-10-07","2024-10-08","2024-10-10","2024-10-11","2024-10-15","2024-10-17","2024-10-18","2024-10-21","2024-10-22","2024-10-24","2024-10-28","2024-10-30","2024-10-31","2024-11-01","2024-11-05","2024-11-06","2024-11-07","2024-11-08","2024-11-11","2024-11-12","2024-11-14","2024-11-15","2024-11-18","2024-11-19","2024-11-25","2024-11-26","2024-11-27","2024-11-29","2024-12-02","2024-12-03","2024-12-09","2024-12-10","2024-12-11","2024-12-13","2024-12-16","2024-12-17","2024-12-18","2024-12-19","2024-12-20","2024-12-23","2024-12-26","2024-12-27","2024-12-31","2025-01-02","2025-01-03","2025-01-07","2025-01-10","2025-01-13","2025-01-14","2025-01-15","2025-01-17","2025-01-21","2025-01-23","2025-01-24","2025-01-27","2025-01-29"],"y":[-0.06012077251849768,-0.32945366944881826,-0.22158669324914013,-0.12093706137323121,-0.07944033040820223,-0.05802021210220365,-0.08808781516765339,-0.09453477629305514,-0.2933136341139002,-0.2446368784264261,0.0526017503795004,0.6284005475383391,0.9145485905422528,1.0901039936083465,1.1998849322177738,0.8728919184287621,0.4727311605960487,0.11766047819684725,-0.05479206658933253,-0.30377911659406515,-0.29862767877112334,0.3238459440337883,0.7357743718636285,1.278345395432236,2.1085012666549785,2.2112421290468602,2.0835037979953395,1.8305462089803708,1.7492761129248349,1.5877262230912415,1.0766596072383245,0.9404857134490238,0.934562559584814,0.9415173338565276,0.7217955154892114,-1.1771071023118507,-1.7622731193981802,-1.4305565103583266,-1.0553239133542436,-0.3742571836580299,-0.33886753887793586,0.2510833842328232,0.1526969196074518,-0.26002454975010014,-0.7167218378736475,-1.0789645479378729,-1.2571696426966166,-1.044566538278158,-0.6975115731020578,-0.4735406515576075,-0.32988817623543903,-0.4053204203725977,-0.17817866556014383,-0.1062291399304719,-0.14696481759747387,-0.3622522676240493,-0.5226065973729949,-0.4609919356188913,-0.5845078203513799,-0.46133580908805927,-0.03813552882130072,0.5189048373103118,1.8514385214964952,2.0707924752223796,2.256946645059017,2.3957622334835085,2.6285325838784503,2.111254920094104,1.5263887926611166,1.0050712820355443,-0.34088146450547824,-0.6821292331499853,-0.8449266639414925,-0.8903339559860891,-0.9012912574977425,-0.7156702248226001,-0.5187466843237551,-0.2800838354513878,0.1461819834939794,0.5764263385070039,0.6366921632425189,0.644834500541459,0.6482932278766632,0.5868131832558573,0.2190751618995032,0.011414238968134921,-0.19836723702308987,-1.094842705450135,-1.4443327746113754,-1.5771772268271302,-1.6930066065337244,-1.7412435240629764,-1.5418568898780263,-1.0885130345002694,-0.7221665011354295,-0.49384251466736945,-0.20711868352905435,0.3472306727799017,0.48186380017246744,0.7619700662937351,1.0185983046656168,1.2032596904500459,1.022700795123312,0.6208299998450992,0.605950141667756,0.6700039059356822,0.8375595858935357,1.2298938901954934,1.2989658079960018,1.4684207890916814,1.8054007947584112,2.4664415598259413,2.3393989727701587,1.8629129660547818,0.4157427965608513,-0.32920442782582315,-0.4241374964875919,-0.6035274885877425,-0.9169028464292477,-1.0667117231237975,-0.6815747705020277,-0.4276895228260496,-0.2584251382656161,-0.5156185020286772,-0.7470160242327671,-1.105754690303043,-1.2940454073143788,-1.4365103196093207,-1.4233738199500618,-1.1124348989365975,-1.1029903741206954,-0.8423929295814782,-0.7065402509319821,-0.6021933128410959,-0.44319181863945895,-0.294542515271152,-0.2285028150766481,-0.29686432760947645,-0.4290090373007542,-0.27031409696698,-0.09717647092826505,0.10782543041015058,0.2825464738185072,0.5467892436994548,0.6592918475183228,1.6044427387899418,1.611012321462481,1.6924890081680866,1.1814703746597155,0.9917588471130003,0.9862515402347493,1.3058872611951113,1.7102891251475378,1.5140586629586301,1.7279159725494857,1.2270002882514686,0.6216790207747782,0.08310119454578906,-0.3923745172485659,-0.6635867226731691,-0.7121741778860907,-0.6188951672678541,-0.6421839313249132,-0.47324342759008525,0.12502710092581834,0.3539521995434626,0.45466580787875993,0.27934898369243943,0.24158821138657444,0.4044642799093932,0.49736742882877716,0.3934989813403376,0.1313927586340889,-0.1413868700453136,0.14941279469265129,0.18867034957940634,0.0994884980472633,-0.1141209027635165,-0.12500356507254048,-0.15523644065807232,0.7217155449149806,0.9346078762435405,0.9868020043539868,1.701881552110741,1.7693180716683607,1.5872424011171744,0.7605772454593205,0.4069540304535816,-0.4149715358385691,-1.6468049401184968,-1.6212316833244373,-1.1812482452137705,-0.9534587287501245,0.2629216949038403,0.46311203335976203,1.5375740103093023,1.4912337282320758,1.0138920363724988,0.7385186954024324,0.29786083878247127,-0.35517487180517926,-0.7089993460893862,-0.8903566143154523,-1.0732319906088903,-0.9876941315755772,-0.8999983998811212,-0.6839552279281202,-0.4894774541576186,-0.5854221505833478,-0.4659167901482201,-0.45606574824668106,-0.5621587105414715,-0.10822840428607226,0.1835708926296928,0.3985864427039099,-0.07874858494116517,-0.7574255273109648,-0.47628097656769136,-0.20857414797993812,0.8761840404010027,1.067593609807076,1.2381308593406062,1.1452397060073447,0.03651566937426848,-0.38743899997601516,-1.2193062414871765,-1.955532674754652,-2.2870306975583135,-2.512129204199322,-2.855482864631744,-2.280943604016941,-1.806762082682127,-1.1865249702696883,-0.46457861587287236,-0.19868045510547344,0.12577749148062342,-0.38094538934898664,-1.2233927378300293,-1.574178330293711,-1.686169122437547,-1.1104849497684246,-0.5243086377610541,0.3526979943710468,-0.03227235288774049,-0.34174781239291896,-0.5290522256554773,-0.8744451357306241,-1.0215949908322024,-1.0591984876754252,-1.2845369060391074,-1.3267587035437776,-0.7259970916407852,-0.19003830171762376,0.721347680273549,0.6504497676949056,0.6683991630795776,0.5109051142020716,0.6134033991854597,0.7985139530284706,0.7982820383632182,-0.41019196118575996,-0.6923628009649168,-0.9556885749280768,-1.0310248543761622,-1.0462672458233355,-1.2503015032136358,-1.6101877470240513,-1.8042896590956812,-1.8695016638469688,-1.3954174400441282,-0.39305959850108213,0.2015696032017924,0.18048136277882054,0.34352936803622325,0.5062788164831844,0.3866881541022244,-0.19676915838150613,-0.31736478431188675,-0.2860296476449796,0.0339965962861961,0.07908933740552775,0.13611102251332927,0.524881304890688,0.7831596027773816,0.9910750987033845,1.1292989048786324,1.1062780422452303,0.42737318373887956,-0.2026230044147448,-0.6342601802621839,-1.6570878231208368,-1.606366486419017,-1.744663598953975,-1.8975513427561752,-1.7718202731176762,-1.6365940306329474,0.5870877488940164,1.045205847552962,1.2862105014071972,1.5929562986455137,1.4189389962903354,1.1185868464612732,0.8874692212669627,0.6353206678945671,0.2986712072679447,0.015943239155048562,-0.17546099887942315,-0.2664421883316179,-0.12767991362325484,0.03022598371150871,0.23955296027256925,0.2607025114691099,0.22897951751720877,0.2140450127807986,0.28855093109984814,0.337607547014895,0.1665424916915094,0.01076514447400457,-0.271291070815417,-0.32184980068297436,-0.17662990210600657,0.48171852029596673]},{"line":{"color":"orange"},"mode":"lines","name":"Smoothed Momentum","type":"scatter","x":["2023-04-03","2023-04-05","2023-04-06","2023-04-11","2023-04-12","2023-04-13","2023-04-17","2023-04-18","2023-04-19","2023-04-20","2023-04-25","2023-04-26","2023-04-27","2023-04-28","2023-05-01","2023-05-04","2023-05-05","2023-05-08","2023-05-09","2023-05-10","2023-05-12","2023-05-15","2023-05-16","2023-05-17","2023-05-19","2023-05-23","2023-05-24","2023-05-25","2023-05-26","2023-05-30","2023-06-01","2023-06-02","2023-06-06","2023-06-07","2023-06-08","2023-06-15","2023-06-22","2023-06-23","2023-06-26","2023-06-28","2023-06-29","2023-07-05","2023-07-07","2023-07-10","2023-07-11","2023-07-12","2023-07-13","2023-07-14","2023-07-18","2023-07-19","2023-07-20","2023-07-21","2023-07-24","2023-07-25","2023-07-26","2023-07-27","2023-07-31","2023-08-01","2023-08-02","2023-08-03","2023-08-04","2023-08-07","2023-08-10","2023-08-11","2023-08-15","2023-08-16","2023-08-17","2023-08-18","2023-08-21","2023-08-22","2023-08-28","2023-08-30","2023-08-31","2023-09-06","2023-09-07","2023-09-08","2023-09-11","2023-09-12","2023-09-13","2023-09-14","2023-09-15","2023-09-18","2023-09-19","2023-09-20","2023-09-21","2023-09-22","2023-09-25","2023-09-27","2023-09-28","2023-09-29","2023-10-02","2023-10-03","2023-10-04","2023-10-06","2023-10-10","2023-10-11","2023-10-12","2023-10-17","2023-10-18","2023-10-20","2023-10-23","2023-10-24","2023-10-25","2023-10-26","2023-10-27","2023-10-30","2023-10-31","2023-11-01","2023-11-02","2023-11-06","2023-11-08","2023-11-09","2023-11-10","2023-11-13","2023-11-15","2023-11-16","2023-11-17","2023-11-24","2023-11-27","2023-11-28","2023-11-30","2023-12-01","2023-12-05","2023-12-07","2023-12-08","2023-12-12","2023-12-13","2023-12-14","2023-12-15","2023-12-18","2023-12-19","2023-12-20","2023-12-21","2023-12-22","2023-12-26","2023-12-27","2023-12-28","2024-01-08","2024-01-09","2024-01-10","2024-01-11","2024-01-12","2024-01-16","2024-01-17","2024-01-18","2024-01-23","2024-01-24","2024-01-25","2024-01-29","2024-01-31","2024-02-01","2024-02-02","2024-02-05","2024-02-14","2024-02-15","2024-02-16","2024-02-20","2024-02-21","2024-02-22","2024-02-23","2024-02-26","2024-02-28","2024-02-29","2024-03-01","2024-03-05","2024-03-06","2024-03-07","2024-03-08","2024-03-13","2024-03-14","2024-03-15","2024-03-18","2024-03-20","2024-03-22","2024-03-25","2024-03-26","2024-03-27","2024-03-28","2024-04-01","2024-04-02","2024-04-05","2024-04-08","2024-04-09","2024-04-12","2024-04-17","2024-04-18","2024-04-23","2024-04-24","2024-04-26","2024-05-01","2024-05-02","2024-05-06","2024-05-07","2024-05-09","2024-05-10","2024-05-14","2024-05-15","2024-05-16","2024-05-17","2024-05-20","2024-05-21","2024-05-22","2024-05-23","2024-05-28","2024-05-29","2024-05-30","2024-06-04","2024-06-06","2024-06-07","2024-06-10","2024-06-11","2024-06-13","2024-06-14","2024-06-18","2024-06-20","2024-06-24","2024-06-27","2024-06-28","2024-07-01","2024-07-03","2024-07-08","2024-07-09","2024-07-10","2024-07-12","2024-07-15","2024-07-16","2024-07-17","2024-07-18","2024-07-19","2024-07-25","2024-07-29","2024-07-30","2024-07-31","2024-08-01","2024-08-02","2024-08-05","2024-08-07","2024-08-09","2024-08-12","2024-08-13","2024-08-19","2024-08-20","2024-08-22","2024-08-26","2024-08-27","2024-08-28","2024-08-29","2024-08-30","2024-09-03","2024-09-04","2024-09-05","2024-09-10","2024-09-11","2024-09-13","2024-09-16","2024-09-18","2024-09-19","2024-09-20","2024-09-23","2024-09-24","2024-09-27","2024-09-30","2024-10-01","2024-10-02","2024-10-03","2024-10-04","2024-10-07","2024-10-08","2024-10-10","2024-10-11","2024-10-15","2024-10-17","2024-10-18","2024-10-21","2024-10-22","2024-10-24","2024-10-28","2024-10-30","2024-10-31","2024-11-01","2024-11-05","2024-11-06","2024-11-07","2024-11-08","2024-11-11","2024-11-12","2024-11-14","2024-11-15","2024-11-18","2024-11-19","2024-11-25","2024-11-26","2024-11-27","2024-11-29","2024-12-02","2024-12-03","2024-12-09","2024-12-10","2024-12-11","2024-12-13","2024-12-16","2024-12-17","2024-12-18","2024-12-19","2024-12-20","2024-12-23","2024-12-26","2024-12-27","2024-12-31","2025-01-02","2025-01-03","2025-01-07","2025-01-10","2025-01-13","2025-01-14","2025-01-15","2025-01-17","2025-01-21","2025-01-23","2025-01-24","2025-01-27","2025-01-29"],"y":[-0.18823120030566193,-0.4854500367922776,-0.5401401297354261,-0.4117629882488861,-0.2913205910587394,-0.2824157091390264,-0.7538648455178107,-0.8919131644008313,-1.1092277668748893,-0.9820132805944998,-0.15237385678914805,0.26546856990672396,0.3329761815266152,0.25602199635687944,0.09141757560644077,-0.1292698594217934,-0.19445971033852233,-0.17544199303816854,-0.04202164585228031,0.10837799716865609,0.1835089275833058,0.47029217862395134,0.48221128191412327,0.5496924695520432,0.7992103563233045,0.601913807609447,0.5495490250785724,0.2911715546423755,0.25230753946454115,0.3254963072053275,0.3086849923984167,0.24960096874118817,0.3726178167448846,0.44954180307809316,0.6187855200893854,1.9548160215578827,1.5945363521293985,1.3688264731019537,1.1239969557008793,0.6534537844177775,0.4247617702837058,0.46000381250603467,0.50057594937468,0.4455027086931476,0.5848533524012336,0.6765823183398256,0.6058792923303657,0.7350208418663955,0.5542657058580085,0.3884571065011979,0.2604480133347914,0.013321496930692215,-0.22635345656136854,-0.39312841868691606,-0.561894616595687,-0.877089310522615,-1.4307038187986205,-1.6710186101395061,-1.9409244864183528,-1.6387228419054434,-1.162381494009947,-0.7216370246659141,0.409760498240882,0.4576483032572335,0.5105604397001001,0.6251480374593269,0.5390473796776872,0.15300621526780184,-0.17606295661416055,-0.5167700051211871,-1.1414008523355923,-0.9068861250344059,-0.5380280986060938,0.14770254460345672,0.2963921785062774,0.43242283762413003,0.5863179959726025,0.8167615426250382,0.9498383780454596,1.0457159091514117,0.9474375705418809,0.9674952602749983,0.8508126182319942,0.8945216592444584,0.7632510917335682,0.6694893666115044,0.11213209003393801,-0.9741389339711071,-1.5241163698733837,-1.9459525926994239,-1.856886786656712,-1.8255328446366927,-1.6242689237746117,-0.7688340312968026,-0.28382184246464565,0.05712679673457183,0.13026649136599808,-0.30555368019749096,-0.6508660512742881,-1.1091201835197797,-1.1255124894699189,-1.3672919243821735,-1.4871605436783566,-1.5893062207901127,-1.47239520004589,-1.5442703183979165,-1.5846650371019813,-1.4272045286165287,-0.9564612900414575,-0.0795531373685221,0.8095213715686367,1.0145639218506664,0.9026451462691536,0.9130410957421802,1.044088946833739,0.799993638645627,0.6930840950723793,0.13201047417519685,0.28332363200095423,0.3632807139774277,0.10552231968994494,0.07505924334738462,-0.024700725679043117,0.4515009525975383,0.544804032889887,0.5879770445540802,0.5713525850481664,0.7103710415545067,0.7230941888671119,0.7488198227301253,0.8028606406880324,0.7082155995976748,0.5738609759068548,0.46944661120132336,0.40915274665586226,0.41809915197579683,0.4691974707999939,-1.2297871845565622,-1.3213462820397122,-1.4004275977570304,-1.4970015895303062,-1.6141372141211647,-1.5886248595410661,-1.6283646409780559,-1.4526111872687686,-1.0329002075630331,-1.0467690232362068,-1.3733165922685087,-1.7500527401747423,-1.8508205953701553,-1.7611262760422863,-1.5831664198395292,-1.5147547305512603,0.6895206323626504,0.5291251744525993,0.292921199733408,0.3298543768010465,0.33445403708895644,0.24623191104156794,0.14052843350197022,0.14228562830214128,0.08852603700936863,0.19328768833492294,0.21118238640207,0.22478318740113515,0.00821034384915114,-0.18370326225447398,-0.19773062181942241,-0.9197017562838438,-1.2826181616262637,-1.6394419517001286,-1.679011864681676,-1.3205177014626124,-0.6732377268559583,-0.5587577124518412,-0.3985906332430089,-0.32766111595960834,-0.24807208230324154,-0.10133216078344956,-0.005660359251300605,-0.34201500044444194,-0.4200242574651915,-0.4851121873086713,-0.765470635879043,-0.5776433098424186,-0.4309222625954327,-0.5615397802665257,-0.634643613779591,-0.7643551663549747,-0.014904978081897871,0.2823214081138563,0.7810193321727726,0.9432890053745477,0.7528400428422647,0.563385754487941,-0.5233609011923205,-0.8787313722585646,-1.2613015578955806,-1.5249034270502622,-1.5979449754629682,-1.5957800963697466,-1.6072216805573973,-1.6595468271153282,-1.5945230697994588,-1.6550566376098061,-1.4923585184148698,-0.9366263167276981,-0.6603560356516321,-0.40084988370036784,-0.14458066953894744,0.021569554156025378,0.11524634505036821,-0.11736774297714957,-0.3832477368620497,-0.4003648148887349,-0.286830401553244,-0.07829233594367836,0.07565189551415417,0.3397048597893856,0.7364382997557091,0.9094134605048677,0.8744450952385081,1.090330915259585,0.8417057815930339,0.9496892712901281,1.0734271165158416,0.8349997524576255,0.7175866760562147,0.6057679341207002,-0.02263021794995918,-0.16702784221194908,-0.3433022258512571,-0.5032145023769262,-0.7377311171053905,-0.9614837340406949,-1.2305119566331082,-1.1122231139725154,-0.8968563364542081,-1.1291137007252867,-1.3386785267766437,-1.439293500362116,-1.0769338860681046,-0.05423330037125436,0.5344458319624994,0.6944789038342775,0.8784804147688978,0.7712311342847659,0.6717127559505023,0.5217849697502266,0.4789290458695289,0.2565316017232058,0.3888723405033868,0.58846211336574,1.258410089637449,1.5191015459222255,1.578374312307729,1.2657767183217663,1.1238082129726044,1.0334023335591616,0.9452519297485219,0.9497213575539333,0.9675046974114148,1.0145639218506664,0.9814735467305867,0.8274311690540608,0.7201611268698181,0.40012140710820665,0.022290551378015493,-0.5960457258486143,-1.0020955068822837,-1.3852696692497637,-1.2502619957191465,-1.13113891019962,-1.0198939461580154,-0.8638150344434886,-0.7429347541237644,0.00918048147247069,0.29437074388652673,0.43414794616049596,0.5224399070173285,1.0529258813712858,1.5975279243724418,1.6267396364266018,1.6543810089815607,1.7355686935463508,1.8312310579420834,2.134151812249705,2.2531012544592333,2.4122038246806357,2.3666998403223345,1.1732304963697688,0.8825515954176485,0.5101791793889944,0.39078052948619396,0.3499120665341639,0.44250547416824365,1.3847676839315164,1.5847745782233362,1.8425631713473463,2.144024944365433,2.320467309032905,2.6504065848921887,2.7905876966046526,2.7285668360955477,2.3314974340729226,1.9202402411663233,0.8914847887466107,0.44348693635523073,-0.3817944178543751,-1.0244577453275643,-1.196002236196437,-1.461574579434258,-1.1825920653529516,-1.210471253745519,-1.1567928218259116,-1.061751421006171,-0.5595466570560244,-0.3079412757100973,0.08380746880262775,0.2399939638722911,0.2823761435050502,0.34842477383540627]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-04-27"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-07-14"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["L"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-07-26"],"y":[-0.3212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-08-04"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-09-14"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["L"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-09-28"],"y":[-0.3212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-10-12"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["L"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-10-26"],"y":[-0.3212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2023-12-20"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["L"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-02-20"],"y":[-0.3212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-05-09"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-07-08"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["L"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-08-05"],"y":[-0.3212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-09-24"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["L"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-10-07"],"y":[-0.3212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-11-14"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["H"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2024-12-16"],"y":[-0.1212940390645619]},{"mode":"text","showlegend":false,"text":["L"],"textfont":{"color":"red","size":12},"type":"scatter","x":["2025-01-14"],"y":[-0.3212940390645619]}],"layout":{"hovermode":"x unified","template":{"data":{"bar":[{"error_x":{"color":"#f2f5fa"},"error_y":{"color":"#f2f5fa"},"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"bar"}],"barpolar":[{"marker":{"line":{"color":"rgb(17,17,17)","width":0.5},"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"barpolar"}],"carpet":[{"aaxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"baxis":{"endlinecolor":"#A2B1C6","gridcolor":"#506784","linecolor":"#506784","minorgridcolor":"#506784","startlinecolor":"#A2B1C6"},"type":"carpet"}],"choropleth":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"choropleth"}],"contour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"contour"}],"contourcarpet":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"contourcarpet"}],"heatmap":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"heatmap"}],"histogram":[{"marker":{"pattern":{"fillmode":"overlay","size":10,"solidity":0.2}},"type":"histogram"}],"histogram2d":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2d"}],"histogram2dcontour":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"histogram2dcontour"}],"mesh3d":[{"colorbar":{"outlinewidth":0,"ticks":""},"type":"mesh3d"}],"parcoords":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"parcoords"}],"pie":[{"automargin":true,"type":"pie"}],"scatter":[{"marker":{"line":{"color":"#283442"}},"type":"scatter"}],"scatter3d":[{"line":{"colorbar":{"outlinewidth":0,"ticks":""}},"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatter3d"}],"scattercarpet":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattercarpet"}],"scattergeo":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattergeo"}],"scattergl":[{"marker":{"line":{"color":"#283442"}},"type":"scattergl"}],"scattermap":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermap"}],"scattermapbox":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scattermapbox"}],"scatterpolar":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolar"}],"scatterpolargl":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterpolargl"}],"scatterternary":[{"marker":{"colorbar":{"outlinewidth":0,"ticks":""}},"type":"scatterternary"}],"surface":[{"colorbar":{"outlinewidth":0,"ticks":""},"colorscale":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"type":"surface"}],"table":[{"cells":{"fill":{"color":"#506784"},"line":{"color":"rgb(17,17,17)"}},"header":{"fill":{"color":"#2a3f5f"},"line":{"color":"rgb(17,17,17)"}},"type":"table"}]},"layout":{"annotationdefaults":{"arrowcolor":"#f2f5fa","arrowhead":0,"arrowwidth":1},"autotypenumbers":"strict","coloraxis":{"colorbar":{"outlinewidth":0,"ticks":""}},"colorscale":{"diverging":[[0,"#8e0152"],[0.1,"#c51b7d"],[0.2,"#de77ae"],[0.3,"#f1b6da"],[0.4,"#fde0ef"],[0.5,"#f7f7f7"],[0.6,"#e6f5d0"],[0.7,"#b8e186"],[0.8,"#7fbc41"],[0.9,"#4d9221"],[1,"#276419"]],"sequential":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]],"sequentialminus":[[0,"#0d0887"],[0.1111111111111111,"#46039f"],[0.2222222222222222,"#7201a8"],[0.3333333333333333,"#9c179e"],[0.4444444444444444,"#bd3786"],[0.5555555555555556,"#d8576b"],[0.6666666666666666,"#ed7953"],[0.7777777777777778,"#fb9f3a"],[0.8888888888888888,"#fdca26"],[1,"#f0f921"]]},"colorway":["#636efa","#EF553B","#00cc96","#ab63fa","#FFA15A","#19d3f3","#FF6692","#B6E880","#FF97FF","#FECB52"],"font":{"color":"#f2f5fa"},"geo":{"bgcolor":"rgb(17,17,17)","lakecolor":"rgb(17,17,17)","landcolor":"rgb(17,17,17)","showlakes":true,"showland":true,"subunitcolor":"#506784"},"hoverlabel":{"align":"left"},"hovermode":"closest","mapbox":{"style":"dark"},"paper_bgcolor":"rgb(17,17,17)","plot_bgcolor":"rgb(17,17,17)","polar":{"angularaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","radialaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"scene":{"xaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"yaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"},"zaxis":{"backgroundcolor":"rgb(17,17,17)","gridcolor":"#506784","gridwidth":2,"linecolor":"#506784","showbackground":true,"ticks":"","zerolinecolor":"#C8D4E3"}},"shapedefaults":{"line":{"color":"#f2f5fa"}},"sliderdefaults":{"bgcolor":"#C8D4E3","bordercolor":"rgb(17,17,17)","borderwidth":1,"tickwidth":0},"ternary":{"aaxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"baxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""},"bgcolor":"rgb(17,17,17)","caxis":{"gridcolor":"#506784","linecolor":"#506784","ticks":""}},"title":{"x":0.05},"updatemenudefaults":{"bgcolor":"#506784","borderwidth":0},"xaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2},"yaxis":{"automargin":true,"gridcolor":"#283442","linecolor":"#506784","ticks":"","title":{"standoff":15},"zerolinecolor":"#283442","zerolinewidth":2}}},"title":{"text":"Momentum and Smoothed Momentum with H/L Labels"},"xaxis":{"rangeslider":{"visible":true},"title":{"text":"Time (t)"}},"yaxis":{"autorange":true,"title":{"text":"Momentum / Ratio / Events"}}}}},"metadata":{},"output_type":"display_data"}],"execution_count":12},{"id":"3c00ae1f","cell_type":"code","source":"except_features = ['train_id', 'ticker_id', 't', 'class_label']\n\ndfs = [df_001, df_002, df_003, df_004, df_005, df_006]\n\nmax_lag = 2\n\nfor i, df in enumerate(dfs, start=1):\n    cols_to_lag = [col for col in df.columns if col not in except_features]\n    \n    for col in cols_to_lag:\n        for lag in range(1, max_lag + 1):\n            df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n    \n    globals()[f\"df_{i:03}\"] = df","metadata":{},"outputs":[],"execution_count":13},{"id":"8af93d49","cell_type":"markdown","source":"# AdaBoost","metadata":{}},{"id":"d00d5383","cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\n\nn_estimators = 50\nexcept_features = ['train_id', 'ticker_id', 't', 'class_label']\n\ndfs = [df_001, df_002, df_003, df_004, df_005, df_006]\n\nclass_mapping = {'N': 0, 'H': 1, 'L': 2}\ninverse_class_mapping = {v: k for k, v in class_mapping.items()}\n\nmodels = {}\n\nfor df in dfs:\n    ticker_id = df['ticker_id'].iloc[0]\n\n    df_HL = df[df['class_label'].isin(['H', 'L'])]\n    df_HL_oversampled = pd.concat([df_HL] * 4, ignore_index=True)\n    n_hl = 2*len(df_HL_oversampled)\n    df_N = df[df['class_label'] == 'N']\n    df_N_sampled = df_N.sample(n=n_hl, replace=False)  \n\n    # Combine and sort by time\n    train_df = pd.concat([df_HL_oversampled, df_N_sampled]).sort_values('t').reset_index(drop=True)\n\n\n    X_train = train_df.drop(except_features, axis=1)\n    y_train = train_df['class_label']\n\n    base_estimator = DecisionTreeClassifier(max_depth=4, random_state=None)\n    ada = AdaBoostClassifier(estimator=base_estimator, n_estimators=n_estimators, random_state=42)\n\n    X_train = X_train.fillna(0)\n    ada.fit(X_train, y_train)\n\n    models[ticker_id] = ada\n\n    validation_set_size =  3*n_hl\n    val_df = df.sample(n=validation_set_size, replace=True, random_state=99)\n    X_val = val_df.drop(except_features, axis=1)\n    y_val = val_df['class_label']\n\n    X_val = X_val.fillna(0)\n    y_pred = ada.predict(X_val)\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_val, y_pred, labels=['H', 'L', 'N'])\n    print(f\"Confusion Matrix for Ticker {ticker_id}:\\n{cm}\\n\")\n\n    # Classification Report\n    cr = classification_report(y_val, y_pred, labels=['H', 'L', 'N'])\n    print(f\"Classification Report for Ticker {ticker_id}:\\n{cr}\")\n\n    # Balanced Accuracy\n    bal_acc = balanced_accuracy_score(y_val, y_pred)\n    print(f\"Balanced Accuracy for Ticker {ticker_id}: {bal_acc:.4f}\\n\")\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix for Ticker 1:\n","[[ 17   0   0]\n"," [  0   8   0]\n"," [  4   4 399]]\n","\n","Classification Report for Ticker 1:\n","              precision    recall  f1-score   support\n","\n","           H       0.81      1.00      0.89        17\n","           L       0.67      1.00      0.80         8\n","           N       1.00      0.98      0.99       407\n","\n","    accuracy                           0.98       432\n","   macro avg       0.83      0.99      0.89       432\n","weighted avg       0.99      0.98      0.98       432\n","\n","Balanced Accuracy for Ticker 1: 0.9934\n","\n","Confusion Matrix for Ticker 2:\n","[[  6   0   0]\n"," [  0   4   0]\n"," [  0   0 326]]\n","\n","Classification Report for Ticker 2:\n","              precision    recall  f1-score   support\n","\n","           H       1.00      1.00      1.00         6\n","           L       1.00      1.00      1.00         4\n","           N       1.00      1.00      1.00       326\n","\n","    accuracy                           1.00       336\n","   macro avg       1.00      1.00      1.00       336\n","weighted avg       1.00      1.00      1.00       336\n","\n","Balanced Accuracy for Ticker 2: 1.0000\n","\n","Confusion Matrix for Ticker 3:\n","[[ 19   0   0]\n"," [  0  21   0]\n"," [  2   0 510]]\n","\n","Classification Report for Ticker 3:\n","              precision    recall  f1-score   support\n","\n","           H       0.90      1.00      0.95        19\n","           L       1.00      1.00      1.00        21\n","           N       1.00      1.00      1.00       512\n","\n","    accuracy                           1.00       552\n","   macro avg       0.97      1.00      0.98       552\n","weighted avg       1.00      1.00      1.00       552\n","\n","Balanced Accuracy for Ticker 3: 0.9987\n","\n","Confusion Matrix for Ticker 4:\n","[[  9   0   0]\n"," [  0   8   0]\n"," [  0   0 343]]\n","\n","Classification Report for Ticker 4:\n","              precision    recall  f1-score   support\n","\n","           H       1.00      1.00      1.00         9\n","           L       1.00      1.00      1.00         8\n","           N       1.00      1.00      1.00       343\n","\n","    accuracy                           1.00       360\n","   macro avg       1.00      1.00      1.00       360\n","weighted avg       1.00      1.00      1.00       360\n","\n","Balanced Accuracy for Ticker 4: 1.0000\n","\n","Confusion Matrix for Ticker 5:\n","[[ 13   0   0]\n"," [  0  18   0]\n"," [  6   0 467]]\n","\n","Classification Report for Ticker 5:\n","              precision    recall  f1-score   support\n","\n","           H       0.68      1.00      0.81        13\n","           L       1.00      1.00      1.00        18\n","           N       1.00      0.99      0.99       473\n","\n","    accuracy                           0.99       504\n","   macro avg       0.89      1.00      0.94       504\n","weighted avg       0.99      0.99      0.99       504\n","\n","Balanced Accuracy for Ticker 5: 0.9958\n","\n","Confusion Matrix for Ticker 6:\n","[[ 21   0   0]\n"," [  0  12   0]\n"," [  0   6 465]]\n","\n","Classification Report for Ticker 6:\n","              precision    recall  f1-score   support\n","\n","           H       1.00      1.00      1.00        21\n","           L       0.67      1.00      0.80        12\n","           N       1.00      0.99      0.99       471\n","\n","    accuracy                           0.99       504\n","   macro avg       0.89      1.00      0.93       504\n","weighted avg       0.99      0.99      0.99       504\n","\n","Balanced Accuracy for Ticker 6: 0.9958\n","\n"]}],"execution_count":14},{"id":"6fb17d4d","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\n# Load test data\ntest_df = pd.read_csv('modified_test.csv')\n\n# Create index-to-feature mapping from columns\nfeature_mapping = {idx: col for idx, col in enumerate(test_df.columns)}\n\nexcept_features_test = ['id', 'ticker_id', 't']\nmax_lag = 2\n\n# Define lag feature order (same as training)\nlag_feature_order = [col for col in test_df.columns if col not in except_features_test]\n\n# Initialize lag buffers for each ticker_id\nlag_buffers = {\n    tid: {col: [] for col in lag_feature_order} for tid in range(1, 7)\n}\n\n# Assume these are already loaded from training\n# feature_stats = {ticker_id: {'mean': ..., 'std': ...}, ...}\n# models = {ticker_id: [list of trained decision trees], ...}\n\n# Class label mappings\nclass_mapping = {'N': 0, 'H': 1, 'L': 2}\ninverse_class_mapping = {v: k for k, v in class_mapping.items()}\n\ndef forest_predict(models, X, class_mapping, inverse_class_mapping):\n    \"\"\"\n    Predict using a custom ensemble of trees with majority voting.\n    \"\"\"\n    preds = np.array([\n        [class_mapping[label] for label in tree.predict(X)]\n        for tree in models\n    ])\n    final_preds = []\n    for i in range(X.shape[0]):\n        counts = np.bincount(preds[:, i])\n        majority_class_int = np.argmax(counts)\n        final_preds.append(inverse_class_mapping[majority_class_int])\n    return final_preds\n\npredictions = []\n\nfor idx, row in test_df.iterrows():\n    ticker = int(row['ticker_id'])\n    row_id = row['id']\n\n    if ticker not in models or ticker not in feature_stats:\n        print(f\"Skipping row id {row_id} due to missing model or stats for ticker {ticker}\")\n        continue\n\n    stats = feature_stats[ticker]\n    model = models[ticker]\n\n    # Scale features\n    scaled_row = {}\n    for idx_col, col in feature_mapping.items():\n        if col not in except_features_test:\n            val = row[col]\n            mean = stats['mean'].get(col, 0)\n            std = stats['std'].get(col, 1)\n            if pd.isna(val):\n                val = mean  # simple imputation\n            scaled_row[col] = (val - mean) / std if std != 0 else 0\n\n    # Create lag features\n    lag_features = {}\n    for col in lag_feature_order:\n        for lag in range(1, max_lag + 1):\n            if len(lag_buffers[ticker][col]) >= lag:\n                lag_features[f'{col}_lag{lag}'] = lag_buffers[ticker][col][-lag]\n            else:\n                lag_features[f'{col}_lag{lag}'] = 0\n\n    # Prepare input feature vector following the order used in training\n    input_features = []\n    for col in lag_feature_order:\n        input_features.append(scaled_row[col])\n    for col in lag_feature_order:\n        for lag in range(1, max_lag + 1):\n            input_features.append(lag_features[f'{col}_lag{lag}'])\n\n    input_array = np.array(input_features).reshape(1, -1)\n\n    # Predict using the custom forest predictor\n    pred = forest_predict(model, input_array, class_mapping, inverse_class_mapping)[0]\n\n    predictions.append({'id': row_id, 'class_label': pred})\n\n    # Update the lag buffers\n    for col in lag_feature_order:\n        lag_buffers[ticker][col].append(scaled_row[col])\n        if len(lag_buffers[ticker][col]) > max_lag:\n            lag_buffers[ticker][col].pop(0)\n\n# Save predictions\nos.makedirs('predictions', exist_ok=True)\npd.DataFrame(predictions).to_csv('predictions/adaboost_predictions.csv', index=False)\n","metadata":{},"outputs":[],"execution_count":15},{"id":"75cb3d8d","cell_type":"markdown","source":"# Custom ensemble","metadata":{}},{"id":"ba7fbd2b","cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\nfrom collections import Counter\n\ndef build_small_nn(input_dim, mid_dim=16, out_dim=3):\n    model = keras.Sequential([\n        keras.layers.Dense(mid_dim, activation='relu', input_shape=(input_dim,)),\n        keras.layers.Dense(out_dim, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\n# Ensemble configuration\nn_ensemble = 30        # models per ticker\nn_cols = 16               # input features per member\nn_rows = 40               # rows per member\nmid_dim = 8           # hidden size\nepochs = 10\nexcept_features = ['train_id', 'ticker_id', 't', 'class_label']\nclass_mapping = {'N': 0, 'H': 1, 'L': 2}\ninverse_class_mapping = {v: k for k, v in class_mapping.items()}\n\ndfs = [df_001, df_002, df_003, df_004, df_005, df_006]\n\nmodels = {}\ncol_subspaces = {}\nweights = {}\n\nfor df in dfs:\n    ticker_id = df['ticker_id'].iloc[0]\n    # Imbalance handling (optional, edit repeat factor as needed)\n    df_HL = df[df['class_label'].isin(['H', 'L'])]\n    df_HL_oversampled = pd.concat([df_HL], ignore_index=True)  # upsample HL\n    n_hl = 2 * len(df_HL_oversampled)\n    df_N = df[df['class_label'] == 'N']\n    df_N_sampled = df_N.sample(n=n_hl, replace=False)  # downsample N\n    \n    # Merge and prep X/y\n    train_df = pd.concat([df_HL_oversampled, df_N_sampled]).sort_values('t').reset_index(drop=True)\n    y_full = train_df['class_label'].map(class_mapping).values\n    candidate_cols = [col for col in train_df.columns if col not in except_features]\n\n    ensemble = []\n    subspaces = []\n    val_scores = []\n\n    # Build ensemble: for each NN, sample rows/cols\n    for i in range(n_ensemble):\n        chosen_cols = np.random.choice(candidate_cols, size=n_cols, replace=False)\n        subspaces.append(chosen_cols)\n        # Draw bootstrap row samples\n        rows = np.random.choice(len(train_df), size=n_rows, replace=True)\n        X_boot = train_df.iloc[rows][chosen_cols].fillna(0).values\n        y_boot = y_full[rows]\n\n        # If any class in y_boot has <2, use unstratified split (prevents ValueError)\n        class_counts = Counter(y_boot)\n        if min(class_counts.values()) < 2:\n            X_train, X_val, y_train, y_val = train_test_split(X_boot, y_boot, test_size=0.2)\n        else:\n            X_train, X_val, y_train, y_val = train_test_split(X_boot, y_boot, test_size=0.2, stratify=y_boot)\n\n        # Build, train, evaluate\n        model = build_small_nn(input_dim=n_cols, mid_dim=mid_dim, out_dim=3)\n        model.fit(X_train, y_train, epochs=epochs, batch_size=5, verbose=0)\n        val_pred = np.argmax(model.predict(X_val, verbose=0), axis=1)\n        bal_acc = balanced_accuracy_score(y_val, val_pred)\n        val_scores.append(bal_acc)\n        ensemble.append(model)\n\n    col_subspaces[ticker_id] = subspaces\n    models[ticker_id] = ensemble\n    # Normalize validation balanced accuracy for weighted voting\n    arr = np.array(val_scores)\n    arr = arr / (arr.sum() + 1e-12)\n    weights[ticker_id] = arr.tolist()\n\n    # --- ENSEMBLE VALIDATION ---\n    # Sample a large validation set from the original df (not train_df)\n    validation_set_size = 3 * n_hl\n    val_df = df.sample(n=validation_set_size, replace=True, random_state=99)\n    X_val_full = val_df.drop(except_features, axis=1).fillna(0)\n    y_val_full = val_df['class_label'].map(class_mapping).values\n\n    # For each row, build ensemble prediction\n    y_pred_full = []\n    for _, row in X_val_full.iterrows():\n        weighted_probs = np.zeros(3)\n        for model, subcols, w in zip(ensemble, subspaces, weights[ticker_id]):\n            input_features = []\n            for col in subcols:\n                # If missing, fill with 0\n                input_features.append(row[col] if col in row else 0)\n            input_array = np.array(input_features, dtype=np.float32).reshape(1, -1)\n            prob = model.predict(input_array, verbose=0)[0]\n            weighted_probs += w * prob\n        pred_class_idx = int(np.argmax(weighted_probs))\n        y_pred_full.append(pred_class_idx)\n\n    # Print metrics\n    cm = confusion_matrix(y_val_full, y_pred_full, labels=[1, 2, 0])\n    print(f\"Confusion Matrix for Ticker {ticker_id}:\\n{cm}\\n\")\n    cr = classification_report(y_val_full, y_pred_full, labels=[1, 2, 0], target_names=['H', 'L', 'N'])\n    print(f\"Classification Report for Ticker {ticker_id}:\\n{cr}\")\n    bal_acc = balanced_accuracy_score(y_val_full, y_pred_full)\n    print(f\"Balanced Accuracy for Ticker {ticker_id}: {bal_acc:.4f}\\n\")\n\nprint('Finished NN ensemble training and validation!')","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-10-05 10:49:40.219042: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7946cc75c8b0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7946cc6f4e50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","Confusion Matrix for Ticker 1:\n","[[ 0  0  3]\n"," [ 0  0  2]\n"," [ 4  0 99]]\n","\n","Classification Report for Ticker 1:\n","              precision    recall  f1-score   support\n","\n","           H       0.00      0.00      0.00         3\n","           L       0.00      0.00      0.00         2\n","           N       0.95      0.96      0.96       103\n","\n","    accuracy                           0.92       108\n","   macro avg       0.32      0.32      0.32       108\n","weighted avg       0.91      0.92      0.91       108\n","\n","Balanced Accuracy for Ticker 1: 0.3204\n","\n","Confusion Matrix for Ticker 2:\n","[[ 0  0  2]\n"," [ 0  1  1]\n"," [ 7  6 67]]\n","\n","Classification Report for Ticker 2:\n","              precision    recall  f1-score   support\n","\n","           H       0.00      0.00      0.00         2\n","           L       0.14      0.50      0.22         2\n","           N       0.96      0.84      0.89        80\n","\n","    accuracy                           0.81        84\n","   macro avg       0.37      0.45      0.37        84\n","weighted avg       0.91      0.81      0.86        84\n","\n","Balanced Accuracy for Ticker 2: 0.4458\n","\n","Confusion Matrix for Ticker 3:\n","[[  0   0   2]\n"," [  0   0   2]\n"," [  0  11 123]]\n","\n","Classification Report for Ticker 3:\n","              precision    recall  f1-score   support\n","\n","           H       0.00      0.00      0.00         2\n","           L       0.00      0.00      0.00         2\n","           N       0.97      0.92      0.94       134\n","\n","    accuracy                           0.89       138\n","   macro avg       0.32      0.31      0.31       138\n","weighted avg       0.94      0.89      0.92       138\n","\n","Balanced Accuracy for Ticker 3: 0.3060\n","\n","Confusion Matrix for Ticker 4:\n","[[ 1  0  3]\n"," [ 0  0  0]\n"," [ 2  1 83]]\n","\n","Classification Report for Ticker 4:\n","              precision    recall  f1-score   support\n","\n","           H       0.33      0.25      0.29         4\n","           L       0.00      0.00      0.00         0\n","           N       0.97      0.97      0.97        86\n","\n","    accuracy                           0.93        90\n","   macro avg       0.43      0.41      0.42        90\n","weighted avg       0.94      0.93      0.93        90\n","\n","Balanced Accuracy for Ticker 4: 0.6076\n","\n","Confusion Matrix for Ticker 5:\n","[[  0   0   3]\n"," [  0   1   1]\n"," [ 17   4 100]]\n","\n","Classification Report for Ticker 5:\n","              precision    recall  f1-score   support\n","\n","           H       0.00      0.00      0.00         3\n","           L       0.20      0.50      0.29         2\n","           N       0.96      0.83      0.89       121\n","\n","    accuracy                           0.80       126\n","   macro avg       0.39      0.44      0.39       126\n","weighted avg       0.93      0.80      0.86       126\n","\n","Balanced Accuracy for Ticker 5: 0.4421\n","\n","Confusion Matrix for Ticker 6:\n","[[  0   0   7]\n"," [  0   0   3]\n"," [  2   1 113]]\n","\n","Classification Report for Ticker 6:\n","              precision    recall  f1-score   support\n","\n","           H       0.00      0.00      0.00         7\n","           L       0.00      0.00      0.00         3\n","           N       0.92      0.97      0.95       116\n","\n","    accuracy                           0.90       126\n","   macro avg       0.31      0.32      0.32       126\n","weighted avg       0.85      0.90      0.87       126\n","\n","Balanced Accuracy for Ticker 6: 0.3247\n","\n","Finished NN ensemble training and validation!\n"]}],"execution_count":16},{"id":"f2165235","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\ntest_df = pd.read_csv('modified_test.csv')\nexcept_features_test = ['id', 'ticker_id', 't']\nmax_lag = 2\nlag_feature_order = [col for col in test_df.columns if col not in except_features_test]\nclass_mapping = {'N': 0, 'H': 1, 'L': 2}\ninverse_class_mapping = {v: k for k, v in class_mapping.items()}\n\n# Prepare lag buffers\nlag_buffers = {tid: {col: [] for col in lag_feature_order} for tid in range(1, 7)}\n\n# Prepare output\npredictions = []\n\n# Group test data by ticker for efficiency\nfor ticker, group in test_df.groupby('ticker_id'):\n    if (ticker not in models or ticker not in feature_stats or\n        ticker not in col_subspaces or ticker not in weights):\n        for row_id in group['id']:\n            print(f\"Skipping row id {row_id} due to missing ensemble/model/stats for ticker {ticker}\")\n        continue\n\n    stats = feature_stats[ticker]\n    ensemble_models = models[ticker]\n    model_cols = col_subspaces[ticker]\n    model_weights = weights[ticker]\n\n    # Precompute scaled features and lag features for all rows\n    scaled_rows = []\n    lag_features_list = []\n    for idx, row in group.iterrows():\n        scaled_row = {}\n        for col in lag_feature_order:\n            val = row.get(col, np.nan)\n            mean = stats['mean'].get(col, 0)\n            std = stats['std'].get(col, 1)\n            if pd.isna(val):\n                val = mean\n            scaled_row[col] = (val - mean) / std if std != 0 else 0\n        scaled_rows.append(scaled_row)\n\n        lag_features = {}\n        for col in lag_feature_order:\n            for lag in range(1, max_lag + 1):\n                key = f'{col}_lag{lag}'\n                if len(lag_buffers[ticker][col]) >= lag:\n                    lag_features[key] = lag_buffers[ticker][col][-lag]\n                else:\n                    lag_features[key] = 0\n        lag_features_list.append(lag_features)\n\n        # Update lag buffers for next row\n        for col in lag_feature_order:\n            lag_buffers[ticker][col].append(scaled_row.get(col, 0))\n            if len(lag_buffers[ticker][col]) > max_lag:\n                lag_buffers[ticker][col].pop(0)\n\n    # For each model in the ensemble, build its input matrix for all rows\n    all_weighted_probs = np.zeros((len(group), 3))\n    for model, subcols, w in zip(ensemble_models, model_cols, model_weights):\n        input_matrix = []\n        for scaled_row, lag_features in zip(scaled_rows, lag_features_list):\n            input_features = []\n            for col in subcols:\n                if col in scaled_row:\n                    input_features.append(scaled_row[col])\n                elif col in lag_features:\n                    input_features.append(lag_features[col])\n                else:\n                    input_features.append(0)\n            input_matrix.append(input_features)\n        input_matrix = np.array(input_matrix, dtype=np.float32)\n        probs = model.predict(input_matrix, verbose=0)\n        all_weighted_probs += w * probs\n\n    # Final prediction for each row in this ticker group\n    for i, (idx, row) in enumerate(group.iterrows()):\n        final_class_idx = int(np.argmax(all_weighted_probs[i]))\n        final_class = inverse_class_mapping[final_class_idx]\n        predictions.append({'id': row['id'], 'class_label': final_class})\n\n# Save predictions\nos.makedirs('predictions', exist_ok=True)\npd.DataFrame(predictions).to_csv('predictions/nn_ensemble_predictions.csv', index=False)","metadata":{},"outputs":[],"execution_count":17},{"id":"b5e873f0","cell_type":"markdown","source":"# XgBoost","metadata":{}},{"id":"cf8f55f0","cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, balanced_accuracy_score\nimport pandas as pd\nimport numpy as np\n\nn_estimators = 30\nexcept_features = ['train_id', 'ticker_id', 't', 'class_label']\n\ndfs = [df_001, df_002, df_003, df_004, df_005, df_006]\n\nclass_mapping = {'N': 0, 'H': 1, 'L': 2}\ninverse_class_mapping = {v: k for k, v in class_mapping.items()}\n\nmodels = {}\n\nfor df in dfs:\n    ticker_id = df['ticker_id'].iloc[0]\n\n    # Oversample HL by repeating 2 times\n    df_HL = df[df['class_label'].isin(['H', 'L'])]\n    df_HL_oversampled = pd.concat([df_HL] * 2, ignore_index=True)\n\n    # Undersample N to match twice the oversampled HL size\n    n_hl = 3 * len(df_HL_oversampled)\n    df_N = df[df['class_label'] == 'N']\n    df_N_sampled = df_N.sample(n=n_hl, replace=False)\n\n    # Combine and sort by time\n    train_df = pd.concat([df_HL_oversampled, df_N_sampled]).sort_values('t').reset_index(drop=True)\n\n    X_train = train_df.drop(except_features, axis=1)\n    y_train = train_df['class_label'].map(class_mapping).astype(int)  # Encode labels as integers\n\n    xgb = XGBClassifier(\n        n_estimators=n_estimators,\n        max_depth=2,\n        use_label_encoder=False,\n        eval_metric='mlogloss',\n        random_state=42\n    )\n\n    X_train = X_train.fillna(0)\n    xgb.fit(X_train, y_train)\n\n    models[ticker_id] = xgb\n\n    validation_set_size = 3 * n_hl\n    val_df = df.sample(n=validation_set_size, replace=True, random_state=99)\n    X_val = val_df.drop(except_features, axis=1).fillna(0)\n    y_val = val_df['class_label'].map(class_mapping).astype(int)  # Encode validation labels\n\n    y_pred = xgb.predict(X_val)\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_val, y_pred, labels=[1, 2, 0])\n    print(f\"Confusion Matrix for Ticker {ticker_id}:\\n{cm}\\n\")\n\n    # Classification Report\n    cr = classification_report(y_val, y_pred, labels=[1, 2, 0], target_names=['H', 'L', 'N'])\n    print(f\"Classification Report for Ticker {ticker_id}:\\n{cr}\")\n\n    # Balanced Accuracy\n    bal_acc = balanced_accuracy_score(y_val, y_pred)\n    print(f\"Balanced Accuracy for Ticker {ticker_id}: {bal_acc:.4f}\\n\")\n","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Confusion Matrix for Ticker 1:\n","[[ 12   0   0]\n"," [  0   5   0]\n"," [  7   2 298]]\n","\n","Classification Report for Ticker 1:\n","              precision    recall  f1-score   support\n","\n","           H       0.63      1.00      0.77        12\n","           L       0.71      1.00      0.83         5\n","           N       1.00      0.97      0.99       307\n","\n","    accuracy                           0.97       324\n","   macro avg       0.78      0.99      0.86       324\n","weighted avg       0.98      0.97      0.97       324\n","\n","Balanced Accuracy for Ticker 1: 0.9902\n","\n","Confusion Matrix for Ticker 2:\n","[[  5   0   0]\n"," [  0   4   0]\n"," [  1   4 238]]\n","\n","Classification Report for Ticker 2:\n","              precision    recall  f1-score   support\n","\n","           H       0.83      1.00      0.91         5\n","           L       0.50      1.00      0.67         4\n","           N       1.00      0.98      0.99       243\n","\n","    accuracy                           0.98       252\n","   macro avg       0.78      0.99      0.86       252\n","weighted avg       0.99      0.98      0.98       252\n","\n","Balanced Accuracy for Ticker 2: 0.9931\n","\n","Confusion Matrix for Ticker 3:\n","[[ 14   0   0]\n"," [  0  13   0]\n"," [ 13  10 364]]\n","\n","Classification Report for Ticker 3:\n","              precision    recall  f1-score   support\n","\n","           H       0.52      1.00      0.68        14\n","           L       0.57      1.00      0.72        13\n","           N       1.00      0.94      0.97       387\n","\n","    accuracy                           0.94       414\n","   macro avg       0.69      0.98      0.79       414\n","weighted avg       0.97      0.94      0.95       414\n","\n","Balanced Accuracy for Ticker 3: 0.9802\n","\n","Confusion Matrix for Ticker 4:\n","[[  5   0   0]\n"," [  0   5   0]\n"," [  3   5 252]]\n","\n","Classification Report for Ticker 4:\n","              precision    recall  f1-score   support\n","\n","           H       0.62      1.00      0.77         5\n","           L       0.50      1.00      0.67         5\n","           N       1.00      0.97      0.98       260\n","\n","    accuracy                           0.97       270\n","   macro avg       0.71      0.99      0.81       270\n","weighted avg       0.98      0.97      0.97       270\n","\n","Balanced Accuracy for Ticker 4: 0.9897\n","\n","Confusion Matrix for Ticker 5:\n","[[  9   0   0]\n"," [  0  11   0]\n"," [  8   3 347]]\n","\n","Classification Report for Ticker 5:\n","              precision    recall  f1-score   support\n","\n","           H       0.53      1.00      0.69         9\n","           L       0.79      1.00      0.88        11\n","           N       1.00      0.97      0.98       358\n","\n","    accuracy                           0.97       378\n","   macro avg       0.77      0.99      0.85       378\n","weighted avg       0.98      0.97      0.97       378\n","\n","Balanced Accuracy for Ticker 5: 0.9898\n","\n","Confusion Matrix for Ticker 6:\n","[[ 16   0   0]\n"," [  0   8   0]\n"," [  1   6 347]]\n","\n","Classification Report for Ticker 6:\n","              precision    recall  f1-score   support\n","\n","           H       0.94      1.00      0.97        16\n","           L       0.57      1.00      0.73         8\n","           N       1.00      0.98      0.99       354\n","\n","    accuracy                           0.98       378\n","   macro avg       0.84      0.99      0.90       378\n","weighted avg       0.99      0.98      0.98       378\n","\n","Balanced Accuracy for Ticker 6: 0.9934\n","\n"]}],"execution_count":18},{"id":"0fb9df2d","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\n\n# Load test data\ntest_df = pd.read_csv('modified_test.csv')\n\n# Create index-to-feature mapping from columns\nfeature_mapping = {idx: col for idx, col in enumerate(test_df.columns)}\n\nexcept_features_test = ['id', 'ticker_id', 't']\nmax_lag = 2\n\n# Define lag feature order (same as training)\nlag_feature_order = [col for col in test_df.columns if col not in except_features_test]\n\n# Initialize lag buffers for each ticker_id\nlag_buffers = {\n    tid: {col: [] for col in lag_feature_order} for tid in range(1, 7)\n}\n\n# Assume these are already loaded from training:\n# feature_stats = {ticker_id: {'mean': ..., 'std': ...}, ...}\n# models = {ticker_id: XGBClassifier trained model, ...}\n\n# Class label mappings\nclass_mapping = {'N': 0, 'H': 1, 'L': 2}\ninverse_class_mapping = {v: k for k, v in class_mapping.items()}\n\npredictions = []\n\nfor idx, row in test_df.iterrows():\n    ticker = int(row['ticker_id'])\n    row_id = row['id']\n\n    if ticker not in models or ticker not in feature_stats:\n        print(f\"Skipping row id {row_id} due to missing model or stats for ticker {ticker}\")\n        continue\n\n    stats = feature_stats[ticker]\n    model = models[ticker]\n\n    # Scale features with simple mean imputation for missing values\n    scaled_row = {}\n    for idx_col, col in feature_mapping.items():\n        if col not in except_features_test:\n            val = row[col]\n            mean = stats['mean'].get(col, 0)\n            std = stats['std'].get(col, 1)\n            if pd.isna(val):\n                val = mean\n            scaled_row[col] = (val - mean) / std if std != 0 else 0\n\n    # Create lag features\n    lag_features = {}\n    for col in lag_feature_order:\n        for lag in range(1, max_lag + 1):\n            if len(lag_buffers[ticker][col]) >= lag:\n                lag_features[f'{col}_lag{lag}'] = lag_buffers[ticker][col][-lag]\n            else:\n                lag_features[f'{col}_lag{lag}'] = 0\n\n    # Prepare input feature vector\n    input_features = [scaled_row[col] for col in lag_feature_order]\n    input_features += [lag_features[f'{col}_lag{lag}'] for col in lag_feature_order for lag in range(1, max_lag + 1)]\n\n    input_array = np.array(input_features).reshape(1, -1)\n\n    # Predict directly with XGBoost model\n    pred_int = model.predict(input_array)[0]\n    pred = inverse_class_mapping[pred_int]\n\n    predictions.append({'id': row_id, 'class_label': pred})\n\n    # Update lag buffers\n    for col in lag_feature_order:\n        lag_buffers[ticker][col].append(scaled_row[col])\n        if len(lag_buffers[ticker][col]) > max_lag:\n            lag_buffers[ticker][col].pop(0)\n\n# Save predictions to CSV\nos.makedirs('predictions', exist_ok=True)\npd.DataFrame(predictions).to_csv('predictions/xgboost_predictions.csv', index=False)\n","metadata":{},"outputs":[],"execution_count":19}]}